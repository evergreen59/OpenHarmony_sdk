commit eb3a809cec82e3e2a4274adbe500c67f4c2fb7e4
Author: zhaoxc0502 <zhaoxc0502@thundersoft.com>
Date:   Thu Jun 16 17:18:54 2022 +0800

    linux_drivers_dma_dmabuf
    
    Change-Id: Id353bb3186aa6ec13d1725e87d45094eafef8461

diff --git a/drivers/dma-buf/dma-buf.c b/drivers/dma-buf/dma-buf.c
index b6ae42d59..0b20ab998 100644
--- a/drivers/dma-buf/dma-buf.c
+++ b/drivers/dma-buf/dma-buf.c
@@ -377,6 +377,36 @@ static long dma_buf_ioctl(struct file *file,
 	dmabuf = file->private_data;
 
 	switch (cmd) {
+	case DMA_BUF_IOCTL_PHYS: {
+		struct dma_buf_attachment *attachment = NULL;
+		struct sg_table *sgt = NULL;
+		unsigned long phys = 0;
+		struct device dev;
+
+		if (!dmabuf || IS_ERR(dmabuf)) {
+			return -EFAULT;
+		}
+		memset(&dev, 0, sizeof(dev));
+		device_initialize(&dev);
+		dev.coherent_dma_mask = DMA_BIT_MASK(64);
+		dev.dma_mask = &dev.coherent_dma_mask;
+		arch_setup_dma_ops(&dev, 0, 0, NULL, false);
+		attachment = dma_buf_attach(dmabuf, &dev);
+		if (!attachment || IS_ERR(attachment)) {
+			return -EFAULT;
+		}
+
+		sgt = dma_buf_map_attachment(attachment, DMA_BIDIRECTIONAL);
+		if (sgt && !IS_ERR(sgt)) {
+			phys = sg_dma_address(sgt->sgl);
+			dma_buf_unmap_attachment(attachment, sgt,
+					DMA_BIDIRECTIONAL);
+		}
+		dma_buf_detach(dmabuf, attachment);
+		if (copy_to_user((void __user *) arg, &phys, sizeof(phys)))
+			return -EFAULT;
+		return 0;
+	}
 	case DMA_BUF_IOCTL_SYNC:
 		if (copy_from_user(&sync, (void __user *) arg, sizeof(sync)))
 			return -EFAULT;
diff --git a/drivers/dma/Makefile b/drivers/dma/Makefile
index 948a8da05..908e0c59b 100644
--- a/drivers/dma/Makefile
+++ b/drivers/dma/Makefile
@@ -32,6 +32,7 @@ obj-$(CONFIG_DW_EDMA) += dw-edma/
 obj-$(CONFIG_EP93XX_DMA) += ep93xx_dma.o
 obj-$(CONFIG_FSL_DMA) += fsldma.o
 obj-$(CONFIG_FSL_EDMA) += fsl-edma.o fsl-edma-common.o
+obj-$(CONFIG_FSL_EDMA_V3) += fsl-edma-v3.o
 obj-$(CONFIG_MCF_EDMA) += mcf-edma.o fsl-edma-common.o
 obj-$(CONFIG_FSL_QDMA) += fsl-qdma.o
 obj-$(CONFIG_FSL_RAID) += fsl_raid.o
diff --git a/drivers/dma/fsl-edma-common.c b/drivers/dma/fsl-edma-common.c
index 930ae268c..7bd002c02 100644
--- a/drivers/dma/fsl-edma-common.c
+++ b/drivers/dma/fsl-edma-common.c
@@ -4,6 +4,7 @@
 // Copyright (c) 2017 Sysam, Angelo Dureghello  <angelo@sysam.it>
 
 #include <linux/dmapool.h>
+#include <linux/delay.h>
 #include <linux/module.h>
 #include <linux/slab.h>
 #include <linux/dma-mapping.h>
@@ -160,11 +161,24 @@ EXPORT_SYMBOL_GPL(fsl_edma_free_desc);
 int fsl_edma_terminate_all(struct dma_chan *chan)
 {
 	struct fsl_edma_chan *fsl_chan = to_fsl_edma_chan(chan);
+	struct edma_regs *regs = &fsl_chan->edma->regs;
+	u32 ch = fsl_chan->vchan.chan.chan_id;
 	unsigned long flags;
+	int count = 0;
 	LIST_HEAD(head);
 
-	spin_lock_irqsave(&fsl_chan->vchan.lock, flags);
 	fsl_edma_disable_request(fsl_chan);
+
+	/*
+	 * Checking ACTIVE to ensure minor loop stop indeed to prevent the
+	 * potential illegal memory write if channel not stopped with buffer
+	 * freed.
+	 */
+	while (count++ < EDMA_MINOR_LOOP_TIMEOUT && (EDMA_TCD_CSR_ACTIVE &
+		edma_readw(fsl_chan->edma, &regs->tcd[ch].csr)))
+		udelay(1);
+
+	spin_lock_irqsave(&fsl_chan->vchan.lock, flags);
 	fsl_chan->edesc = NULL;
 	fsl_chan->idle = true;
 	vchan_get_all_descriptors(&fsl_chan->vchan, &head);
@@ -310,6 +324,11 @@ static size_t fsl_edma_desc_residue(struct fsl_edma_chan *fsl_chan,
 	return len;
 }
 
+void fsl_edma_get_realcnt(struct fsl_edma_chan *fsl_chan)
+{
+	fsl_chan->chn_real_count = fsl_edma_desc_residue(fsl_chan, NULL, true);
+}
+
 enum dma_status fsl_edma_tx_status(struct dma_chan *chan,
 		dma_cookie_t cookie, struct dma_tx_state *txstate)
 {
@@ -319,8 +338,12 @@ enum dma_status fsl_edma_tx_status(struct dma_chan *chan,
 	unsigned long flags;
 
 	status = dma_cookie_status(chan, cookie, txstate);
-	if (status == DMA_COMPLETE)
+	if (status == DMA_COMPLETE) {
+		spin_lock_irqsave(&fsl_chan->vchan.lock, flags);
+		txstate->residue = fsl_chan->chn_real_count;
+		spin_unlock_irqrestore(&fsl_chan->vchan.lock, flags);
 		return status;
+	}
 
 	if (!txstate)
 		return fsl_chan->status;
diff --git a/drivers/dma/fsl-edma-common.h b/drivers/dma/fsl-edma-common.h
index ec1169741..be4a17901 100644
--- a/drivers/dma/fsl-edma-common.h
+++ b/drivers/dma/fsl-edma-common.h
@@ -58,6 +58,8 @@
 
 #define DMAMUX_NR	2
 
+#define EDMA_MINOR_LOOP_TIMEOUT		500 /* us */
+
 #define FSL_EDMA_BUSWIDTHS	(BIT(DMA_SLAVE_BUSWIDTH_1_BYTE) | \
 				 BIT(DMA_SLAVE_BUSWIDTH_2_BYTES) | \
 				 BIT(DMA_SLAVE_BUSWIDTH_4_BYTES) | \
@@ -126,6 +128,7 @@ struct fsl_edma_chan {
 	u32				dma_dev_size;
 	enum dma_data_direction		dma_dir;
 	char				chan_name[16];
+	u32				chn_real_count;
 };
 
 struct fsl_edma_desc {
@@ -150,6 +153,7 @@ struct fsl_edma_drvdata {
 	bool			mux_swap;
 	int			(*setup_irq)(struct platform_device *pdev,
 					     struct fsl_edma_engine *fsl_edma);
+	u8			txirq_count;
 };
 
 struct fsl_edma_engine {
@@ -161,7 +165,7 @@ struct fsl_edma_engine {
 	struct mutex		fsl_edma_mutex;
 	const struct fsl_edma_drvdata *drvdata;
 	u32			n_chans;
-	int			txirq;
+	int			*txirqs;
 	int			errirq;
 	bool			big_endian;
 	struct edma_regs	regs;
@@ -182,6 +186,14 @@ static inline u32 edma_readl(struct fsl_edma_engine *edma, void __iomem *addr)
 		return ioread32(addr);
 }
 
+static inline u32 edma_readw(struct fsl_edma_engine *edma, void __iomem *addr)
+{
+	if (edma->big_endian)
+		return ioread16be(addr);
+	else
+		return ioread16(addr);
+}
+
 static inline void edma_writeb(struct fsl_edma_engine *edma,
 			       u8 val, void __iomem *addr)
 {
@@ -230,6 +242,7 @@ int fsl_edma_pause(struct dma_chan *chan);
 int fsl_edma_resume(struct dma_chan *chan);
 int fsl_edma_slave_config(struct dma_chan *chan,
 				 struct dma_slave_config *cfg);
+void fsl_edma_get_realcnt(struct fsl_edma_chan *fsl_chan);
 enum dma_status fsl_edma_tx_status(struct dma_chan *chan,
 		dma_cookie_t cookie, struct dma_tx_state *txstate);
 struct dma_async_tx_descriptor *fsl_edma_prep_dma_cyclic(
diff --git a/drivers/dma/fsl-edma-v3.c b/drivers/dma/fsl-edma-v3.c
new file mode 100644
index 000000000..9a6464c80
--- /dev/null
+++ b/drivers/dma/fsl-edma-v3.c
@@ -0,0 +1,1341 @@
+/*
+ * drivers/dma/fsl-edma3-v3.c
+ *
+ * Copyright 2017-2018 NXP .
+ *
+ * Driver for the Freescale eDMA engine v3. This driver based on fsl-edma3.c
+ * but changed to meet the IP change on i.MX8QM: every dma channel is specific
+ * to hardware. For example, channel 14 for LPUART1 receive request and channel
+ * 13 for transmit requesst. The eDMA block can be found on i.MX8QM
+ *
+ * This program is free software; you can redistribute  it and/or modify it
+ * under  the terms of  the GNU General  Public License as published by the
+ * Free Software Foundation;  either version 2 of the  License, or (at your
+ * option) any later version.
+ */
+
+#include <linux/init.h>
+#include <linux/module.h>
+#include <linux/interrupt.h>
+#include <linux/iopoll.h>
+#include <linux/clk.h>
+#include <linux/dma-mapping.h>
+#include <linux/dmapool.h>
+#include <linux/slab.h>
+#include <linux/spinlock.h>
+#include <linux/of.h>
+#include <linux/of_device.h>
+#include <linux/of_address.h>
+#include <linux/of_irq.h>
+#include <linux/of_dma.h>
+#include <linux/pm_runtime.h>
+#include <linux/pm_domain.h>
+
+#include "virt-dma.h"
+
+#define EDMA_CH_CSR			0x00
+#define EDMA_CH_ES			0x04
+#define EDMA_CH_INT			0x08
+#define EDMA_CH_SBR			0x0C
+#define EDMA_CH_PRI			0x10
+#define EDMA_CH_MUX			0x14
+#define EDMA_TCD_SADDR			0x20
+#define EDMA_TCD_SOFF			0x24
+#define EDMA_TCD_ATTR			0x26
+#define EDMA_TCD_NBYTES			0x28
+#define EDMA_TCD_SLAST			0x2C
+#define EDMA_TCD_DADDR			0x30
+#define EDMA_TCD_DOFF			0x34
+#define EDMA_TCD_CITER_ELINK		0x36
+#define EDMA_TCD_CITER			0x36
+#define EDMA_TCD_DLAST_SGA		0x38
+#define EDMA_TCD_CSR			0x3C
+#define EDMA_TCD_BITER_ELINK		0x3E
+#define EDMA_TCD_BITER			0x3E
+
+#define EDMA_CH_SBR_RD		BIT(22)
+#define EDMA_CH_SBR_WR		BIT(21)
+#define EDMA_CH_CSR_ERQ		BIT(0)
+#define EDMA_CH_CSR_EARQ	BIT(1)
+#define EDMA_CH_CSR_EEI		BIT(2)
+#define EDMA_CH_CSR_DONE	BIT(30)
+#define EDMA_CH_CSR_ACTIVE	BIT(31)
+
+#define EDMA_TCD_ATTR_DSIZE(x)		(((x) & 0x0007))
+#define EDMA_TCD_ATTR_DMOD(x)		(((x) & 0x001F) << 3)
+#define EDMA_TCD_ATTR_SSIZE(x)		(((x) & 0x0007) << 8)
+#define EDMA_TCD_ATTR_SMOD(x)		(((x) & 0x001F) << 11)
+#define EDMA_TCD_ATTR_SSIZE_8BIT	(0x0000)
+#define EDMA_TCD_ATTR_SSIZE_16BIT	(0x0100)
+#define EDMA_TCD_ATTR_SSIZE_32BIT	(0x0200)
+#define EDMA_TCD_ATTR_SSIZE_64BIT	(0x0300)
+#define EDMA_TCD_ATTR_SSIZE_16BYTE	(0x0400)
+#define EDMA_TCD_ATTR_SSIZE_32BYTE	(0x0500)
+#define EDMA_TCD_ATTR_SSIZE_64BYTE	(0x0600)
+#define EDMA_TCD_ATTR_DSIZE_8BIT	(0x0000)
+#define EDMA_TCD_ATTR_DSIZE_16BIT	(0x0001)
+#define EDMA_TCD_ATTR_DSIZE_32BIT	(0x0002)
+#define EDMA_TCD_ATTR_DSIZE_64BIT	(0x0003)
+#define EDMA_TCD_ATTR_DSIZE_16BYTE	(0x0004)
+#define EDMA_TCD_ATTR_DSIZE_32BYTE	(0x0005)
+#define EDMA_TCD_ATTR_DSIZE_64BYTE	(0x0006)
+
+#define EDMA_TCD_SOFF_SOFF(x)		(x)
+#define EDMA_TCD_NBYTES_NBYTES(x)	(x)
+#define EDMA_TCD_NBYTES_MLOFF(x)	(x << 10)
+#define EDMA_TCD_NBYTES_DMLOE		(1 << 30)
+#define EDMA_TCD_NBYTES_SMLOE		(1 << 31)
+#define EDMA_TCD_SLAST_SLAST(x)		(x)
+#define EDMA_TCD_DADDR_DADDR(x)		(x)
+#define EDMA_TCD_CITER_CITER(x)		((x) & 0x7FFF)
+#define EDMA_TCD_DOFF_DOFF(x)		(x)
+#define EDMA_TCD_DLAST_SGA_DLAST_SGA(x)	(x)
+#define EDMA_TCD_BITER_BITER(x)		((x) & 0x7FFF)
+
+#define EDMA_TCD_CSR_START		BIT(0)
+#define EDMA_TCD_CSR_INT_MAJOR		BIT(1)
+#define EDMA_TCD_CSR_INT_HALF		BIT(2)
+#define EDMA_TCD_CSR_D_REQ		BIT(3)
+#define EDMA_TCD_CSR_E_SG		BIT(4)
+#define EDMA_TCD_CSR_E_LINK		BIT(5)
+#define EDMA_TCD_CSR_EEOP		BIT(6)
+#define EDMA_TCD_CSR_ESDA		BIT(7)
+
+#define FSL_EDMA_BUSWIDTHS	(BIT(DMA_SLAVE_BUSWIDTH_1_BYTE) | \
+				BIT(DMA_SLAVE_BUSWIDTH_2_BYTES) | \
+				BIT(DMA_SLAVE_BUSWIDTH_4_BYTES) | \
+				BIT(DMA_SLAVE_BUSWIDTH_8_BYTES) | \
+				BIT(DMA_SLAVE_BUSWIDTH_16_BYTES))
+
+#define ARGS_RX				BIT(0)
+#define ARGS_REMOTE			BIT(1)
+#define ARGS_DFIFO			BIT(2)
+
+/* channel name template define in dts */
+#define CHAN_PREFIX			"edma0-chan"
+#define CHAN_POSFIX			"-tx"
+#define CLK_POSFIX			"-clk"
+
+#define EDMA_MINOR_LOOP_TIMEOUT		500 /* us */
+
+struct fsl_edma3_hw_tcd {
+	__le32	saddr;
+	__le16	soff;
+	__le16	attr;
+	__le32	nbytes;
+	__le32	slast;
+	__le32	daddr;
+	__le16	doff;
+	__le16	citer;
+	__le32	dlast_sga;
+	__le16	csr;
+	__le16	biter;
+	/*
+	 * Store Dest Address if EDMA_TCD_CSR_ESDA enabled which's used as
+	 * 'eeop + cyclic'.
+	 */
+	__le32  sda;
+};
+
+struct fsl_edma3_sw_tcd {
+	dma_addr_t			ptcd;
+	struct fsl_edma3_hw_tcd		*vtcd;
+};
+
+struct fsl_edma3_slave_config {
+	enum dma_transfer_direction	dir;
+	enum dma_slave_buswidth		addr_width;
+	u32				dev_addr;
+	u32				dev2_addr; /* source addr for dev2dev */
+	u32				burst;
+	u32				attr;
+};
+
+struct fsl_edma3_chan {
+	struct virt_dma_chan		vchan;
+	enum dma_status			status;
+	bool				idle;
+	struct fsl_edma3_engine		*edma3;
+	struct fsl_edma3_desc		*edesc;
+	struct fsl_edma3_slave_config	fsc;
+	void __iomem			*membase;
+	int				txirq;
+	int				hw_chanid;
+	int				priority;
+	int				is_rxchan;
+	int				is_remote;
+	int				is_dfifo;
+	struct dma_pool			*tcd_pool;
+	u32				chn_real_count;
+	char                            txirq_name[32];
+	struct platform_device		*pdev;
+	struct device			*dev;
+	struct work_struct		issue_worker;
+	u32				srcid;
+	struct clk			*clk;
+};
+
+struct fsl_edma3_drvdata {
+	bool has_pd;
+	u32 dmamuxs;
+};
+
+struct fsl_edma3_desc {
+	struct virt_dma_desc		vdesc;
+	struct fsl_edma3_chan		*echan;
+	bool				iscyclic;
+	unsigned int			n_tcds;
+	u32				curidx;
+	struct fsl_edma3_sw_tcd		tcd[];
+};
+
+struct fsl_edma3_reg_save {
+	u32 csr;
+	u32 sbr;
+};
+
+struct fsl_edma3_engine {
+	struct dma_device	dma_dev;
+	unsigned long		irqflag;
+	struct mutex		fsl_edma3_mutex;
+	u32			n_chans;
+	int			errirq;
+	#define MAX_CHAN_NUM	32
+	struct fsl_edma3_reg_save edma_regs[MAX_CHAN_NUM];
+	bool			swap;	/* remote/local swapped on Audio edma */
+	const struct fsl_edma3_drvdata *drvdata;
+	struct clk		*clk_mp;
+	struct fsl_edma3_chan	chans[];
+};
+
+
+static struct fsl_edma3_drvdata fsl_edma_imx8q = {
+	.has_pd = true,
+	.dmamuxs = 0,
+};
+
+static struct fsl_edma3_drvdata fsl_edma_imx8ulp = {
+	.has_pd = false,
+	.dmamuxs = 1,
+};
+
+static struct fsl_edma3_chan *to_fsl_edma3_chan(struct dma_chan *chan)
+{
+	return container_of(chan, struct fsl_edma3_chan, vchan.chan);
+}
+
+static struct fsl_edma3_desc *to_fsl_edma3_desc(struct virt_dma_desc *vd)
+{
+	return container_of(vd, struct fsl_edma3_desc, vdesc);
+}
+
+static void fsl_edma3_enable_request(struct fsl_edma3_chan *fsl_chan)
+{
+	void __iomem *addr = fsl_chan->membase;
+	u32 val;
+
+	val = readl(addr + EDMA_CH_SBR);
+	/* Remote/local swapped wrongly on iMX8 QM Audio edma */
+	if (fsl_chan->edma3->swap) {
+		if (!fsl_chan->is_rxchan)
+			val |= EDMA_CH_SBR_RD;
+		else
+			val |= EDMA_CH_SBR_WR;
+	} else {
+		if (fsl_chan->is_rxchan)
+			val |= EDMA_CH_SBR_RD;
+		else
+			val |= EDMA_CH_SBR_WR;
+	}
+
+	if (fsl_chan->is_remote)
+		val &= ~(EDMA_CH_SBR_RD | EDMA_CH_SBR_WR);
+
+	writel(val, addr + EDMA_CH_SBR);
+
+	if (fsl_chan->srcid && !readl(addr + EDMA_CH_MUX))
+		writel(fsl_chan->srcid, addr + EDMA_CH_MUX);
+
+	val = readl(addr + EDMA_CH_CSR);
+
+	val |= EDMA_CH_CSR_ERQ;
+	writel(val, addr + EDMA_CH_CSR);
+}
+
+static void fsl_edma3_disable_request(struct fsl_edma3_chan *fsl_chan)
+{
+	void __iomem *addr = fsl_chan->membase;
+	u32 val = readl(addr + EDMA_CH_CSR);
+
+	if (fsl_chan->srcid)
+		writel(0, addr + EDMA_CH_MUX);
+
+	val &= ~EDMA_CH_CSR_ERQ;
+	writel(val, addr + EDMA_CH_CSR);
+}
+
+static unsigned int fsl_edma3_get_tcd_attr(enum dma_slave_buswidth addr_width)
+{
+	switch (addr_width) {
+	case 1:
+		return EDMA_TCD_ATTR_SSIZE_8BIT | EDMA_TCD_ATTR_DSIZE_8BIT;
+	case 2:
+		return EDMA_TCD_ATTR_SSIZE_16BIT | EDMA_TCD_ATTR_DSIZE_16BIT;
+	case 4:
+		return EDMA_TCD_ATTR_SSIZE_32BIT | EDMA_TCD_ATTR_DSIZE_32BIT;
+	case 8:
+		return EDMA_TCD_ATTR_SSIZE_64BIT | EDMA_TCD_ATTR_DSIZE_64BIT;
+	case 16:
+		return EDMA_TCD_ATTR_SSIZE_16BYTE | EDMA_TCD_ATTR_DSIZE_16BYTE;
+	case 32:
+		return EDMA_TCD_ATTR_SSIZE_32BYTE | EDMA_TCD_ATTR_DSIZE_32BYTE;
+	case 64:
+		return EDMA_TCD_ATTR_SSIZE_64BYTE | EDMA_TCD_ATTR_DSIZE_64BYTE;
+	default:
+		return EDMA_TCD_ATTR_SSIZE_32BIT | EDMA_TCD_ATTR_DSIZE_32BIT;
+	}
+}
+
+static void fsl_edma3_free_desc(struct virt_dma_desc *vdesc)
+{
+	struct fsl_edma3_desc *fsl_desc;
+	int i;
+
+	fsl_desc = to_fsl_edma3_desc(vdesc);
+	for (i = 0; i < fsl_desc->n_tcds; i++)
+		dma_pool_free(fsl_desc->echan->tcd_pool, fsl_desc->tcd[i].vtcd,
+			      fsl_desc->tcd[i].ptcd);
+	kfree(fsl_desc);
+}
+
+static int fsl_edma3_terminate_all(struct dma_chan *chan)
+{
+	struct fsl_edma3_chan *fsl_chan = to_fsl_edma3_chan(chan);
+	unsigned long flags;
+	LIST_HEAD(head);
+	u32 val;
+
+	fsl_edma3_disable_request(fsl_chan);
+
+	/*
+	 * Checking ACTIVE to ensure minor loop stop indeed to prevent the
+	 * potential illegal memory write if channel not stopped with buffer
+	 * freed. Ignore tx channel since no such risk.
+	 */
+	if (fsl_chan->is_rxchan)
+		readl_poll_timeout_atomic(fsl_chan->membase + EDMA_CH_CSR, val,
+			!(val & EDMA_CH_CSR_ACTIVE), 2,
+			EDMA_MINOR_LOOP_TIMEOUT);
+
+	spin_lock_irqsave(&fsl_chan->vchan.lock, flags);
+	fsl_chan->edesc = NULL;
+	fsl_chan->idle = true;
+	fsl_chan->vchan.cyclic = NULL;
+	vchan_get_all_descriptors(&fsl_chan->vchan, &head);
+	spin_unlock_irqrestore(&fsl_chan->vchan.lock, flags);
+	vchan_dma_desc_free_list(&fsl_chan->vchan, &head);
+
+	if (fsl_chan->edma3->drvdata->has_pd)
+		pm_runtime_allow(fsl_chan->dev);
+
+	return 0;
+}
+
+static int fsl_edma3_pause(struct dma_chan *chan)
+{
+	struct fsl_edma3_chan *fsl_chan = to_fsl_edma3_chan(chan);
+	unsigned long flags;
+
+	spin_lock_irqsave(&fsl_chan->vchan.lock, flags);
+	if (fsl_chan->edesc) {
+		fsl_edma3_disable_request(fsl_chan);
+		fsl_chan->status = DMA_PAUSED;
+		fsl_chan->idle = true;
+	}
+	spin_unlock_irqrestore(&fsl_chan->vchan.lock, flags);
+
+	return 0;
+}
+
+static int fsl_edma3_resume(struct dma_chan *chan)
+{
+	struct fsl_edma3_chan *fsl_chan = to_fsl_edma3_chan(chan);
+	unsigned long flags;
+
+	spin_lock_irqsave(&fsl_chan->vchan.lock, flags);
+	if (fsl_chan->edesc) {
+		fsl_edma3_enable_request(fsl_chan);
+		fsl_chan->status = DMA_IN_PROGRESS;
+		fsl_chan->idle = false;
+	}
+	spin_unlock_irqrestore(&fsl_chan->vchan.lock, flags);
+
+	return 0;
+}
+
+static int fsl_edma3_slave_config(struct dma_chan *chan,
+				 struct dma_slave_config *cfg)
+{
+	struct fsl_edma3_chan *fsl_chan = to_fsl_edma3_chan(chan);
+
+	fsl_chan->fsc.dir = cfg->direction;
+	if (cfg->direction == DMA_DEV_TO_MEM) {
+		fsl_chan->fsc.dev_addr = cfg->src_addr;
+		fsl_chan->fsc.addr_width = cfg->src_addr_width;
+		fsl_chan->fsc.burst = cfg->src_maxburst;
+		fsl_chan->fsc.attr = fsl_edma3_get_tcd_attr
+					(cfg->src_addr_width);
+	} else if (cfg->direction == DMA_MEM_TO_DEV) {
+		fsl_chan->fsc.dev_addr = cfg->dst_addr;
+		fsl_chan->fsc.addr_width = cfg->dst_addr_width;
+		fsl_chan->fsc.burst = cfg->dst_maxburst;
+		fsl_chan->fsc.attr = fsl_edma3_get_tcd_attr
+					(cfg->dst_addr_width);
+	} else if (cfg->direction == DMA_DEV_TO_DEV) {
+		fsl_chan->fsc.dev2_addr = cfg->src_addr;
+		fsl_chan->fsc.dev_addr = cfg->dst_addr;
+		fsl_chan->fsc.addr_width = cfg->dst_addr_width;
+		fsl_chan->fsc.burst = cfg->dst_maxburst;
+		fsl_chan->fsc.attr = fsl_edma3_get_tcd_attr
+					(cfg->dst_addr_width);
+	} else {
+			return -EINVAL;
+	}
+	return 0;
+}
+
+static size_t fsl_edma3_desc_residue(struct fsl_edma3_chan *fsl_chan,
+		struct virt_dma_desc *vdesc, bool in_progress)
+{
+	struct fsl_edma3_desc *edesc = fsl_chan->edesc;
+	void __iomem *addr = fsl_chan->membase;
+	enum dma_transfer_direction dir = fsl_chan->fsc.dir;
+	dma_addr_t cur_addr, dma_addr;
+	size_t len, size;
+	int i;
+
+	/* calculate the total size in this desc */
+	for (len = i = 0; i < fsl_chan->edesc->n_tcds; i++)
+		len += le32_to_cpu(edesc->tcd[i].vtcd->nbytes)
+			* le16_to_cpu(edesc->tcd[i].vtcd->biter);
+
+	if (!in_progress)
+		return len;
+
+	if (dir == DMA_MEM_TO_DEV)
+		cur_addr = readl(addr + EDMA_TCD_SADDR);
+	else
+		cur_addr = readl(addr + EDMA_TCD_DADDR);
+
+	/* skip the tcds before curidx in cyclic case */
+	if (edesc->iscyclic)
+		len -= edesc->curidx * le32_to_cpu(edesc->tcd[0].vtcd->nbytes)
+			* le16_to_cpu(edesc->tcd[0].vtcd->biter);
+
+	/* figure out the finished and calculate the residue */
+	for (i = edesc->curidx; i < fsl_chan->edesc->n_tcds; i++) {
+		if (dir == DMA_DEV_TO_MEM)
+			cur_addr = le32_to_cpu(edesc->tcd[i].vtcd->sda);
+
+		size = le32_to_cpu(edesc->tcd[i].vtcd->nbytes)
+			* le16_to_cpu(edesc->tcd[i].vtcd->biter);
+		if (dir == DMA_MEM_TO_DEV)
+			dma_addr = le32_to_cpu(edesc->tcd[i].vtcd->saddr);
+		else
+			dma_addr = le32_to_cpu(edesc->tcd[i].vtcd->daddr);
+
+		len -= size;
+		if (cur_addr > dma_addr && cur_addr <= dma_addr + size) {
+			len += dma_addr + size - cur_addr;
+			/*
+			 * mark curidx as the start point in the next time
+			 * fsl_edma3_desc_residue(called by fsl_edma3_tx_status
+			 * commonly per cyclic interrupt/callback)
+			 */
+			if (edesc->iscyclic)
+				edesc->curidx = (i + 1) % edesc->n_tcds;
+			break;
+		}
+	}
+
+	return len;
+}
+
+static enum dma_status fsl_edma3_tx_status(struct dma_chan *chan,
+		dma_cookie_t cookie, struct dma_tx_state *txstate)
+{
+	struct fsl_edma3_chan *fsl_chan = to_fsl_edma3_chan(chan);
+	struct virt_dma_desc *vdesc;
+	enum dma_status status;
+	unsigned long flags;
+
+	status = dma_cookie_status(chan, cookie, txstate);
+	if (status == DMA_COMPLETE) {
+		spin_lock_irqsave(&fsl_chan->vchan.lock, flags);
+		txstate->residue = fsl_chan->chn_real_count;
+		spin_unlock_irqrestore(&fsl_chan->vchan.lock, flags);
+		return status;
+	}
+
+	if (!txstate)
+		return fsl_chan->status;
+
+	spin_lock_irqsave(&fsl_chan->vchan.lock, flags);
+	vdesc = vchan_find_desc(&fsl_chan->vchan, cookie);
+	if (fsl_chan->edesc && cookie == fsl_chan->edesc->vdesc.tx.cookie)
+		txstate->residue = fsl_edma3_desc_residue(fsl_chan, vdesc,
+								true);
+	else if (fsl_chan->edesc && vdesc)
+		txstate->residue = fsl_edma3_desc_residue(fsl_chan, vdesc,
+								false);
+	else
+		txstate->residue = 0;
+
+	spin_unlock_irqrestore(&fsl_chan->vchan.lock, flags);
+
+	return fsl_chan->status;
+}
+
+static void fsl_edma3_set_tcd_regs(struct fsl_edma3_chan *fsl_chan,
+				  struct fsl_edma3_hw_tcd *tcd)
+{
+	void __iomem *addr = fsl_chan->membase;
+	/*
+	 * TCD parameters are stored in struct fsl_edma3_hw_tcd in little
+	 * endian format. However, we need to load the TCD registers in
+	 * big- or little-endian obeying the eDMA engine model endian.
+	 */
+	writew(0, addr + EDMA_TCD_CSR);
+	writel(le32_to_cpu(tcd->saddr), addr + EDMA_TCD_SADDR);
+	writel(le32_to_cpu(tcd->daddr), addr + EDMA_TCD_DADDR);
+
+	writew(le16_to_cpu(tcd->attr), addr + EDMA_TCD_ATTR);
+	writew(le16_to_cpu(tcd->soff), addr + EDMA_TCD_SOFF);
+
+	writel(le32_to_cpu(tcd->nbytes), addr + EDMA_TCD_NBYTES);
+	writel(le32_to_cpu(tcd->slast), addr + EDMA_TCD_SLAST);
+
+	writew(le16_to_cpu(tcd->citer), addr + EDMA_TCD_CITER);
+	writew(le16_to_cpu(tcd->biter), addr + EDMA_TCD_BITER);
+	writew(le16_to_cpu(tcd->doff), addr + EDMA_TCD_DOFF);
+
+	writel(le32_to_cpu(tcd->dlast_sga), addr + EDMA_TCD_DLAST_SGA);
+
+	/* Must clear CHa_CSR[DONE] bit before enable TCDa_CSR[ESG] */
+	writel(readl(addr + EDMA_CH_CSR), addr + EDMA_CH_CSR);
+
+	writew(le16_to_cpu(tcd->csr), addr + EDMA_TCD_CSR);
+}
+
+static inline
+void fsl_edma3_fill_tcd(struct fsl_edma3_chan *fsl_chan,
+			struct fsl_edma3_sw_tcd *sw_tcd, u32 src, u32 dst,
+			u16 attr, u16 soff, u32 nbytes, u32 slast, u16 citer,
+			u16 biter, u16 doff, u32 dlast_sga, bool major_int,
+			bool disable_req, bool enable_sg)
+{
+	struct fsl_edma3_hw_tcd *tcd = sw_tcd->vtcd;
+	u32 slast_sda = sw_tcd->ptcd + offsetof(struct fsl_edma3_hw_tcd, sda);
+	u16 csr = 0;
+
+	/*
+	 * eDMA hardware SGs require the TCDs to be stored in little
+	 * endian format irrespective of the register endian model.
+	 * So we put the value in little endian in memory, waiting
+	 * for fsl_edma3_set_tcd_regs doing the swap.
+	 */
+	tcd->saddr = cpu_to_le32(src);
+	tcd->daddr = cpu_to_le32(dst);
+
+	tcd->attr = cpu_to_le16(attr);
+
+	tcd->soff = cpu_to_le16(EDMA_TCD_SOFF_SOFF(soff));
+
+	if (fsl_chan->is_dfifo) {
+		/* set mloff as -8 */
+		nbytes |= EDMA_TCD_NBYTES_MLOFF(-8);
+		/* enable DMLOE/SMLOE */
+		if (fsl_chan->fsc.dir == DMA_MEM_TO_DEV) {
+			nbytes |= EDMA_TCD_NBYTES_DMLOE;
+			nbytes &= ~EDMA_TCD_NBYTES_SMLOE;
+		} else {
+			nbytes |= EDMA_TCD_NBYTES_SMLOE;
+			nbytes &= ~EDMA_TCD_NBYTES_DMLOE;
+		}
+	}
+
+	tcd->nbytes = cpu_to_le32(EDMA_TCD_NBYTES_NBYTES(nbytes));
+
+	if (fsl_chan->is_rxchan)
+		tcd->slast = cpu_to_le32(EDMA_TCD_SLAST_SLAST(slast_sda));
+	else
+		tcd->slast = cpu_to_le32(EDMA_TCD_SLAST_SLAST(slast));
+
+	tcd->citer = cpu_to_le16(EDMA_TCD_CITER_CITER(citer));
+	tcd->doff = cpu_to_le16(EDMA_TCD_DOFF_DOFF(doff));
+
+	tcd->dlast_sga = cpu_to_le32(EDMA_TCD_DLAST_SGA_DLAST_SGA(dlast_sga));
+
+	tcd->biter = cpu_to_le16(EDMA_TCD_BITER_BITER(biter));
+	if (major_int)
+		csr |= EDMA_TCD_CSR_INT_MAJOR;
+
+	if (disable_req)
+		csr |= EDMA_TCD_CSR_D_REQ;
+
+	if (enable_sg)
+		csr |= EDMA_TCD_CSR_E_SG;
+
+	if (fsl_chan->is_rxchan)
+		csr |= EDMA_TCD_CSR_EEOP | EDMA_TCD_CSR_ESDA;
+
+	tcd->csr = cpu_to_le16(csr);
+}
+
+static struct fsl_edma3_desc *fsl_edma3_alloc_desc(struct fsl_edma3_chan
+						*fsl_chan, int sg_len)
+{
+	struct fsl_edma3_desc *fsl_desc;
+	int i;
+
+	fsl_desc = kzalloc(sizeof(*fsl_desc) + sizeof(struct fsl_edma3_sw_tcd)
+				* sg_len, GFP_ATOMIC);
+	if (!fsl_desc)
+		return NULL;
+
+	fsl_desc->echan = fsl_chan;
+	fsl_desc->n_tcds = sg_len;
+	for (i = 0; i < sg_len; i++) {
+		fsl_desc->tcd[i].vtcd = dma_pool_alloc(fsl_chan->tcd_pool,
+					GFP_ATOMIC, &fsl_desc->tcd[i].ptcd);
+		if (!fsl_desc->tcd[i].vtcd)
+			goto err;
+	}
+	return fsl_desc;
+
+err:
+	while (--i >= 0)
+		dma_pool_free(fsl_chan->tcd_pool, fsl_desc->tcd[i].vtcd,
+				fsl_desc->tcd[i].ptcd);
+	kfree(fsl_desc);
+	return NULL;
+}
+
+static struct dma_async_tx_descriptor *fsl_edma3_prep_dma_cyclic(
+		struct dma_chan *chan, dma_addr_t dma_addr, size_t buf_len,
+		size_t period_len, enum dma_transfer_direction direction,
+		unsigned long flags)
+{
+	struct fsl_edma3_chan *fsl_chan = to_fsl_edma3_chan(chan);
+	struct fsl_edma3_desc *fsl_desc;
+	dma_addr_t dma_buf_next;
+	int sg_len, i;
+	u32 src_addr, dst_addr, last_sg, nbytes;
+	u16 soff, doff, iter;
+	bool major_int = true;
+
+	sg_len = buf_len / period_len;
+	fsl_desc = fsl_edma3_alloc_desc(fsl_chan, sg_len);
+	if (!fsl_desc)
+		return NULL;
+	fsl_desc->iscyclic = true;
+
+	dma_buf_next = dma_addr;
+	nbytes = fsl_chan->fsc.addr_width * fsl_chan->fsc.burst;
+
+	/*
+	 * Choose the suitable burst length if period_len is not multiple of
+	 * burst length so that the whole transfer length is multiple of minor
+	 * loop(burst length).
+	 */
+	if (period_len % nbytes) {
+		u32 width = fsl_chan->fsc.addr_width;
+
+		for (i = fsl_chan->fsc.burst; i > 1; i--) {
+			if (!(period_len % (i * width))) {
+				nbytes = i * width;
+				break;
+			}
+		}
+		/* if no chance to get suitable burst size, use it as 1 */
+		if (i == 1)
+			nbytes = width;
+	}
+
+	iter = period_len / nbytes;
+
+	for (i = 0; i < sg_len; i++) {
+		if (dma_buf_next >= dma_addr + buf_len)
+			dma_buf_next = dma_addr;
+
+		/* get next sg's physical address */
+		last_sg = fsl_desc->tcd[(i + 1) % sg_len].ptcd;
+
+		if (fsl_chan->fsc.dir == DMA_MEM_TO_DEV) {
+			src_addr = dma_buf_next;
+			dst_addr = fsl_chan->fsc.dev_addr;
+			soff = fsl_chan->fsc.addr_width;
+			if (fsl_chan->is_dfifo)
+				doff = 4;
+			else
+				doff = 0;
+		} else if (fsl_chan->fsc.dir == DMA_DEV_TO_MEM) {
+			src_addr = fsl_chan->fsc.dev_addr;
+			dst_addr = dma_buf_next;
+			if (fsl_chan->is_dfifo)
+				soff = 4;
+			else
+				soff = 0;
+			doff = fsl_chan->fsc.addr_width;
+		} else {
+			/* DMA_DEV_TO_DEV */
+			src_addr = fsl_chan->fsc.dev2_addr;
+			dst_addr = fsl_chan->fsc.dev_addr;
+			soff = 0;
+			doff = 0;
+			major_int = false;
+		}
+
+		fsl_edma3_fill_tcd(fsl_chan, &fsl_desc->tcd[i], src_addr,
+				dst_addr, fsl_chan->fsc.attr, soff, nbytes, 0,
+				iter, iter, doff, last_sg, major_int, false, true);
+		dma_buf_next += period_len;
+	}
+
+	fsl_desc->curidx = 0;
+
+	return vchan_tx_prep(&fsl_chan->vchan, &fsl_desc->vdesc, flags);
+}
+
+static struct dma_async_tx_descriptor *fsl_edma3_prep_slave_sg(
+		struct dma_chan *chan, struct scatterlist *sgl,
+		unsigned int sg_len, enum dma_transfer_direction direction,
+		unsigned long flags, void *context)
+{
+	struct fsl_edma3_chan *fsl_chan = to_fsl_edma3_chan(chan);
+	struct fsl_edma3_desc *fsl_desc;
+	struct scatterlist *sg;
+	u32 src_addr, dst_addr, last_sg, nbytes;
+	u16 soff, doff, iter;
+	int i;
+
+	if (!is_slave_direction(fsl_chan->fsc.dir))
+		return NULL;
+
+	fsl_desc = fsl_edma3_alloc_desc(fsl_chan, sg_len);
+	if (!fsl_desc)
+		return NULL;
+	fsl_desc->iscyclic = false;
+
+	nbytes = fsl_chan->fsc.addr_width * fsl_chan->fsc.burst;
+	for_each_sg(sgl, sg, sg_len, i) {
+		/* get next sg's physical address */
+		last_sg = fsl_desc->tcd[(i + 1) % sg_len].ptcd;
+
+		if (fsl_chan->fsc.dir == DMA_MEM_TO_DEV) {
+			src_addr = sg_dma_address(sg);
+			dst_addr = fsl_chan->fsc.dev_addr;
+			soff = fsl_chan->fsc.addr_width;
+			doff = 0;
+		} else if (fsl_chan->fsc.dir == DMA_DEV_TO_MEM) {
+			src_addr = fsl_chan->fsc.dev_addr;
+			dst_addr = sg_dma_address(sg);
+			soff = 0;
+			doff = fsl_chan->fsc.addr_width;
+		} else {
+			/* DMA_DEV_TO_DEV */
+			src_addr = fsl_chan->fsc.dev2_addr;
+			dst_addr = fsl_chan->fsc.dev_addr;
+			soff = 0;
+			doff = 0;
+		}
+
+		/*
+		 * Choose the suitable burst length if sg_dma_len is not
+		 * multiple of burst length so that the whole transfer length is
+		 * multiple of minor loop(burst length).
+		 */
+		if (sg_dma_len(sg) % nbytes) {
+			u32 width = fsl_chan->fsc.addr_width;
+			int j;
+
+			for (j = fsl_chan->fsc.burst; j > 1; j--) {
+				if (!(sg_dma_len(sg) % (j * width))) {
+					nbytes = j * width;
+					break;
+				}
+			}
+			/* Set burst size as 1 if there's no suitable one */
+			if (j == 1)
+				nbytes = width;
+		}
+
+		iter = sg_dma_len(sg) / nbytes;
+		if (i < sg_len - 1) {
+			last_sg = fsl_desc->tcd[(i + 1)].ptcd;
+			fsl_edma3_fill_tcd(fsl_chan, &fsl_desc->tcd[i],
+					src_addr, dst_addr, fsl_chan->fsc.attr,
+					soff, nbytes, 0, iter, iter, doff,
+					last_sg, false, false, true);
+		} else {
+			last_sg = 0;
+			fsl_edma3_fill_tcd(fsl_chan, &fsl_desc->tcd[i],
+					src_addr, dst_addr, fsl_chan->fsc.attr,
+					soff, nbytes, 0, iter, iter, doff,
+					last_sg, true, true, false);
+		}
+	}
+
+	return vchan_tx_prep(&fsl_chan->vchan, &fsl_desc->vdesc, flags);
+}
+
+static void fsl_edma3_xfer_desc(struct fsl_edma3_chan *fsl_chan)
+{
+	struct virt_dma_desc *vdesc;
+
+	vdesc = vchan_next_desc(&fsl_chan->vchan);
+	if (!vdesc)
+		return;
+	fsl_chan->edesc = to_fsl_edma3_desc(vdesc);
+
+	fsl_edma3_set_tcd_regs(fsl_chan, fsl_chan->edesc->tcd[0].vtcd);
+	fsl_edma3_enable_request(fsl_chan);
+	fsl_chan->status = DMA_IN_PROGRESS;
+	fsl_chan->idle = false;
+}
+
+static size_t fsl_edma3_desc_residue(struct fsl_edma3_chan *fsl_chan,
+		struct virt_dma_desc *vdesc, bool in_progress);
+
+static void fsl_edma3_get_realcnt(struct fsl_edma3_chan *fsl_chan)
+{
+	fsl_chan->chn_real_count = fsl_edma3_desc_residue(fsl_chan, NULL, true);
+}
+
+static irqreturn_t fsl_edma3_tx_handler(int irq, void *dev_id)
+{
+	struct fsl_edma3_chan *fsl_chan = dev_id;
+	unsigned int intr;
+	void __iomem *base_addr;
+
+	spin_lock(&fsl_chan->vchan.lock);
+
+	/* Ignore this interrupt since channel has been freeed with power off */
+	if (!fsl_chan->edesc && !fsl_chan->tcd_pool)
+		goto irq_handled;
+
+	base_addr = fsl_chan->membase;
+
+	intr = readl(base_addr + EDMA_CH_INT);
+	if (!intr)
+		goto irq_handled;
+
+	writel(1, base_addr + EDMA_CH_INT);
+
+	/* Ignore this interrupt since channel has been disabled already */
+	if (!fsl_chan->edesc)
+		goto irq_handled;
+
+	if (!fsl_chan->edesc->iscyclic) {
+		fsl_edma3_get_realcnt(fsl_chan);
+		list_del(&fsl_chan->edesc->vdesc.node);
+		vchan_cookie_complete(&fsl_chan->edesc->vdesc);
+		fsl_chan->edesc = NULL;
+		fsl_chan->status = DMA_COMPLETE;
+		fsl_chan->idle = true;
+	} else {
+		vchan_cyclic_callback(&fsl_chan->edesc->vdesc);
+	}
+
+	if (!fsl_chan->edesc)
+		fsl_edma3_xfer_desc(fsl_chan);
+irq_handled:
+	spin_unlock(&fsl_chan->vchan.lock);
+
+	return IRQ_HANDLED;
+}
+
+static void fsl_edma3_issue_pending(struct dma_chan *chan)
+{
+	struct fsl_edma3_chan *fsl_chan = to_fsl_edma3_chan(chan);
+
+	schedule_work(&fsl_chan->issue_worker);
+}
+
+static struct dma_chan *fsl_edma3_xlate(struct of_phandle_args *dma_spec,
+		struct of_dma *ofdma)
+{
+	struct fsl_edma3_engine *fsl_edma3 = ofdma->of_dma_data;
+	struct dma_chan *chan, *_chan;
+	struct fsl_edma3_chan *fsl_chan;
+
+	if (dma_spec->args_count != 3)
+		return NULL;
+
+	mutex_lock(&fsl_edma3->fsl_edma3_mutex);
+	list_for_each_entry_safe(chan, _chan, &fsl_edma3->dma_dev.channels,
+					device_node) {
+		if (chan->client_count)
+			continue;
+
+		fsl_chan = to_fsl_edma3_chan(chan);
+		if (fsl_edma3->drvdata->dmamuxs == 0 &&
+		    fsl_chan->hw_chanid == dma_spec->args[0]) {
+			chan = dma_get_slave_channel(chan);
+			chan->device->privatecnt++;
+			fsl_chan->priority = dma_spec->args[1];
+			fsl_chan->is_rxchan = dma_spec->args[2] & ARGS_RX;
+			fsl_chan->is_remote = dma_spec->args[2] & ARGS_REMOTE;
+			fsl_chan->is_dfifo = dma_spec->args[2] & ARGS_DFIFO;
+			mutex_unlock(&fsl_edma3->fsl_edma3_mutex);
+			return chan;
+		} else if (fsl_edma3->drvdata->dmamuxs && !fsl_chan->srcid) {
+			chan = dma_get_slave_channel(chan);
+			chan->device->privatecnt++;
+			fsl_chan->priority = dma_spec->args[1];
+			fsl_chan->srcid = dma_spec->args[0];
+			fsl_chan->is_rxchan = dma_spec->args[2] & ARGS_RX;
+			fsl_chan->is_remote = dma_spec->args[2] & ARGS_REMOTE;
+			fsl_chan->is_dfifo = dma_spec->args[2] & ARGS_DFIFO;
+			mutex_unlock(&fsl_edma3->fsl_edma3_mutex);
+			return chan;
+		}
+	}
+	mutex_unlock(&fsl_edma3->fsl_edma3_mutex);
+	return NULL;
+}
+
+static int fsl_edma3_alloc_chan_resources(struct dma_chan *chan)
+{
+	struct fsl_edma3_chan *fsl_chan = to_fsl_edma3_chan(chan);
+	struct platform_device *pdev = fsl_chan->pdev;
+	int ret;
+
+	clk_prepare_enable(fsl_chan->clk);
+
+	fsl_chan->tcd_pool = dma_pool_create("tcd_pool", chan->device->dev,
+				sizeof(struct fsl_edma3_hw_tcd),
+				32, 0);
+
+	if (fsl_chan->edma3->drvdata->has_pd) {
+		pm_runtime_get_sync(fsl_chan->dev);
+		/* clear meaningless pending irq anyway */
+		if (readl(fsl_chan->membase + EDMA_CH_INT))
+			writel(1, fsl_chan->membase + EDMA_CH_INT);
+	}
+
+	ret = devm_request_irq(&pdev->dev, fsl_chan->txirq,
+			fsl_edma3_tx_handler, fsl_chan->edma3->irqflag,
+			fsl_chan->txirq_name, fsl_chan);
+	if (ret) {
+		dev_err(&pdev->dev, "Can't register %s IRQ.\n",
+			fsl_chan->txirq_name);
+		if (fsl_chan->edma3->drvdata->has_pd)
+			pm_runtime_put_sync_suspend(fsl_chan->dev);
+
+		return ret;
+	}
+
+	if (fsl_chan->edma3->drvdata->has_pd) {
+		pm_runtime_mark_last_busy(fsl_chan->dev);
+		pm_runtime_put_autosuspend(fsl_chan->dev);
+	}
+
+	return 0;
+}
+
+static void fsl_edma3_free_chan_resources(struct dma_chan *chan)
+{
+	struct fsl_edma3_chan *fsl_chan = to_fsl_edma3_chan(chan);
+	unsigned long flags;
+	LIST_HEAD(head);
+
+	if (fsl_chan->edma3->drvdata->has_pd)
+		pm_runtime_get_sync(fsl_chan->dev);
+
+	devm_free_irq(&fsl_chan->pdev->dev, fsl_chan->txirq, fsl_chan);
+
+	spin_lock_irqsave(&fsl_chan->vchan.lock, flags);
+	fsl_edma3_disable_request(fsl_chan);
+	fsl_chan->edesc = NULL;
+	vchan_get_all_descriptors(&fsl_chan->vchan, &head);
+	spin_unlock_irqrestore(&fsl_chan->vchan.lock, flags);
+
+	vchan_dma_desc_free_list(&fsl_chan->vchan, &head);
+	dma_pool_destroy(fsl_chan->tcd_pool);
+
+	spin_lock_irqsave(&fsl_chan->vchan.lock, flags);
+	fsl_chan->tcd_pool = NULL;
+	fsl_chan->srcid = 0;
+	/* Clear interrupt before power off */
+	if (readl(fsl_chan->membase + EDMA_CH_INT))
+		writel(1, fsl_chan->membase + EDMA_CH_INT);
+	spin_unlock_irqrestore(&fsl_chan->vchan.lock, flags);
+
+	if (fsl_chan->edma3->drvdata->has_pd)
+		pm_runtime_put_sync_suspend(fsl_chan->dev);
+
+	clk_disable_unprepare(fsl_chan->clk);
+}
+
+static void fsl_edma3_synchronize(struct dma_chan *chan)
+{
+	struct fsl_edma3_chan *fsl_chan = to_fsl_edma3_chan(chan);
+
+	vchan_synchronize(&fsl_chan->vchan);
+}
+
+static struct device *fsl_edma3_attach_pd(struct device *dev,
+					  struct device_node *np, int index)
+{
+	const char *domn = "edma0-chan01";
+	struct device *pd_chan;
+	struct device_link *link;
+	int ret;
+
+	ret = of_property_read_string_index(np, "power-domain-names", index,
+						&domn);
+	if (ret) {
+		dev_err(dev, "parse power-domain-names error.(%d)\n", ret);
+		return NULL;
+	}
+
+	pd_chan = dev_pm_domain_attach_by_name(dev, domn);
+	if (IS_ERR_OR_NULL(pd_chan))
+		return NULL;
+
+	link = device_link_add(dev, pd_chan, DL_FLAG_STATELESS |
+					     DL_FLAG_PM_RUNTIME |
+					     DL_FLAG_RPM_ACTIVE);
+	if (IS_ERR(link)) {
+		dev_err(dev, "Failed to add device_link to %s: %ld\n", domn,
+			PTR_ERR(link));
+		return NULL;
+	}
+
+	return pd_chan;
+}
+
+static void fsl_edma3_issue_work(struct work_struct *work)
+{
+	struct fsl_edma3_chan *fsl_chan = container_of(work,
+						       struct fsl_edma3_chan,
+						       issue_worker);
+	unsigned long flags;
+
+	if (fsl_chan->edma3->drvdata->has_pd)
+		pm_runtime_forbid(fsl_chan->dev);
+
+	spin_lock_irqsave(&fsl_chan->vchan.lock, flags);
+
+	if (vchan_issue_pending(&fsl_chan->vchan) && !fsl_chan->edesc)
+		fsl_edma3_xfer_desc(fsl_chan);
+
+	spin_unlock_irqrestore(&fsl_chan->vchan.lock, flags);
+}
+
+static const struct of_device_id fsl_edma3_dt_ids[] = {
+	{ .compatible = "fsl,imx8qm-edma", .data = &fsl_edma_imx8q},
+	{ .compatible = "fsl,imx8qm-adma", .data = &fsl_edma_imx8q},
+	{ .compatible = "fsl,imx8ulp-edma", .data = &fsl_edma_imx8ulp},
+	{ /* sentinel */ }
+};
+MODULE_DEVICE_TABLE(of, fsl_edma3_dt_ids);
+
+static int fsl_edma3_probe(struct platform_device *pdev)
+{
+	struct device_node *np = pdev->dev.of_node;
+	const struct of_device_id *of_id =
+			of_match_device(fsl_edma3_dt_ids, &pdev->dev);
+	struct fsl_edma3_engine *fsl_edma3;
+	struct fsl_edma3_chan *fsl_chan;
+	struct resource *res_mp;
+	struct resource *res;
+	int len, chans;
+	int ret, i;
+
+	if (!of_id)
+		return -EINVAL;
+
+	ret = of_property_read_u32(np, "dma-channels", &chans);
+	if (ret) {
+		dev_err(&pdev->dev, "Can't get dma-channels.\n");
+		return ret;
+	}
+
+	len = sizeof(*fsl_edma3) + sizeof(*fsl_chan) * chans;
+	fsl_edma3 = devm_kzalloc(&pdev->dev, len, GFP_KERNEL);
+	if (!fsl_edma3)
+		return -ENOMEM;
+
+	/* Audio edma rx/tx channel shared interrupt */
+	if (of_property_read_bool(np, "shared-interrupt"))
+		fsl_edma3->irqflag = IRQF_SHARED;
+
+	fsl_edma3->swap = of_device_is_compatible(np, "fsl,imx8qm-adma");
+	fsl_edma3->n_chans = chans;
+	fsl_edma3->drvdata = (const struct fsl_edma3_drvdata *)of_id->data;
+
+	INIT_LIST_HEAD(&fsl_edma3->dma_dev.channels);
+
+	res_mp = platform_get_resource(pdev, IORESOURCE_MEM, 0);
+
+	fsl_edma3->clk_mp = devm_clk_get_optional(&pdev->dev, "edma-mp-clk");
+	if (IS_ERR(fsl_edma3->clk_mp)) {
+		dev_err(&pdev->dev, "Can't get mp clk.\n");
+		return PTR_ERR(fsl_edma3->clk_mp);
+	}
+	clk_prepare_enable(fsl_edma3->clk_mp);
+
+	for (i = 0; i < fsl_edma3->n_chans; i++) {
+		struct fsl_edma3_chan *fsl_chan = &fsl_edma3->chans[i];
+		const char *txirq_name;
+		char chanid[3], id_len = 0;
+		char clk_name[18];
+		char *p = chanid;
+		unsigned long val;
+
+		fsl_chan->edma3 = fsl_edma3;
+		fsl_chan->pdev = pdev;
+		fsl_chan->idle = true;
+		fsl_chan->srcid = 0;
+		/* Get per channel membase */
+		res = platform_get_resource(pdev, IORESOURCE_MEM, i + 1);
+		fsl_chan->membase = devm_ioremap_resource(&pdev->dev, res);
+		if (IS_ERR(fsl_chan->membase))
+			return PTR_ERR(fsl_chan->membase);
+
+		/* Get the hardware chanel id by the channel membase
+		 * channel0:0x10000, channel1:0x20000... total 32 channels
+		 * Note: skip first res_mp which we don't care.
+		 */
+		fsl_chan->hw_chanid = ((res->start - res_mp->start) >> 16) & 0x3f;
+		fsl_chan->hw_chanid--;
+
+		ret = of_property_read_string_index(np, "interrupt-names", i,
+							&txirq_name);
+		if (ret) {
+			dev_err(&pdev->dev, "read interrupt-names fail.\n");
+			return ret;
+		}
+		/* Get channel id length from dts, one-digit or double-digit */
+		id_len = strlen(txirq_name) - strlen(CHAN_PREFIX) -
+			 strlen(CHAN_POSFIX);
+		if (id_len > 2) {
+			dev_err(&pdev->dev, "%s is edmaX-chanX-tx in dts?\n",
+				res->name);
+			return -EINVAL;
+		}
+		/* Grab channel id from txirq_name */
+		strncpy(p, txirq_name + strlen(CHAN_PREFIX), id_len);
+		*(p + id_len) = '\0';
+
+		/* check if the channel id match well with hw_chanid */
+		ret = kstrtoul(chanid, 0, &val);
+		if (ret || val != fsl_chan->hw_chanid) {
+			dev_err(&pdev->dev, "%s,wrong id?\n", txirq_name);
+			return -EINVAL;
+		}
+
+		/* request channel irq */
+		fsl_chan->txirq = platform_get_irq_byname(pdev, txirq_name);
+		if (fsl_chan->txirq < 0) {
+			dev_err(&pdev->dev, "Can't get %s irq.\n", txirq_name);
+			return fsl_chan->txirq;
+		}
+
+		memcpy(fsl_chan->txirq_name, txirq_name, strlen(txirq_name));
+
+		strncpy(clk_name, txirq_name, strlen(CHAN_PREFIX) + id_len);
+		strcpy(clk_name + strlen(CHAN_PREFIX) + id_len, CLK_POSFIX);
+		fsl_chan->clk = devm_clk_get_optional(&pdev->dev,
+						     (const char *)clk_name);
+		if (IS_ERR(fsl_chan->clk))
+			return PTR_ERR(fsl_chan->clk);
+
+		fsl_chan->vchan.desc_free = fsl_edma3_free_desc;
+		vchan_init(&fsl_chan->vchan, &fsl_edma3->dma_dev);
+
+		INIT_WORK(&fsl_chan->issue_worker,
+				fsl_edma3_issue_work);
+	}
+
+	mutex_init(&fsl_edma3->fsl_edma3_mutex);
+
+	dma_cap_set(DMA_PRIVATE, fsl_edma3->dma_dev.cap_mask);
+	dma_cap_set(DMA_SLAVE, fsl_edma3->dma_dev.cap_mask);
+	dma_cap_set(DMA_CYCLIC, fsl_edma3->dma_dev.cap_mask);
+
+	fsl_edma3->dma_dev.dev = &pdev->dev;
+	fsl_edma3->dma_dev.device_alloc_chan_resources
+		= fsl_edma3_alloc_chan_resources;
+	fsl_edma3->dma_dev.device_free_chan_resources
+		= fsl_edma3_free_chan_resources;
+	fsl_edma3->dma_dev.device_tx_status = fsl_edma3_tx_status;
+	fsl_edma3->dma_dev.device_prep_slave_sg = fsl_edma3_prep_slave_sg;
+	fsl_edma3->dma_dev.device_prep_dma_cyclic = fsl_edma3_prep_dma_cyclic;
+	fsl_edma3->dma_dev.device_config = fsl_edma3_slave_config;
+	fsl_edma3->dma_dev.device_pause = fsl_edma3_pause;
+	fsl_edma3->dma_dev.device_resume = fsl_edma3_resume;
+	fsl_edma3->dma_dev.device_terminate_all = fsl_edma3_terminate_all;
+	fsl_edma3->dma_dev.device_issue_pending = fsl_edma3_issue_pending;
+	fsl_edma3->dma_dev.device_synchronize = fsl_edma3_synchronize;
+
+	fsl_edma3->dma_dev.src_addr_widths = FSL_EDMA_BUSWIDTHS;
+	fsl_edma3->dma_dev.dst_addr_widths = FSL_EDMA_BUSWIDTHS;
+	fsl_edma3->dma_dev.directions = BIT(DMA_DEV_TO_MEM) |
+					BIT(DMA_MEM_TO_DEV) |
+					BIT(DMA_DEV_TO_DEV);
+
+	platform_set_drvdata(pdev, fsl_edma3);
+
+	ret = dma_async_device_register(&fsl_edma3->dma_dev);
+	if (ret) {
+		dev_err(&pdev->dev, "Can't register Freescale eDMA engine.\n");
+		return ret;
+	}
+	/* Attach power domains from dts for each dma chanel device */
+	if (fsl_edma3->drvdata->has_pd) {
+		for (i = 0; i < fsl_edma3->n_chans; i++) {
+			struct fsl_edma3_chan *fsl_chan = &fsl_edma3->chans[i];
+			struct device *dev;
+
+			dev = fsl_edma3_attach_pd(&pdev->dev, np, i);
+			if (!dev) {
+				dev_err(dev, "edma channel attach failed.\n");
+				return -EINVAL;
+			}
+
+			fsl_chan->dev = dev;
+			/* clear meaningless pending irq anyway */
+			writel(1, fsl_chan->membase + EDMA_CH_INT);
+
+			pm_runtime_use_autosuspend(fsl_chan->dev);
+			pm_runtime_set_autosuspend_delay(fsl_chan->dev, 200);
+			pm_runtime_set_active(fsl_chan->dev);
+			pm_runtime_put_sync_suspend(fsl_chan->dev);
+		}
+	}
+
+	ret = of_dma_controller_register(np, fsl_edma3_xlate, fsl_edma3);
+	if (ret) {
+		dev_err(&pdev->dev, "Can't register Freescale eDMA of_dma.\n");
+		dma_async_device_unregister(&fsl_edma3->dma_dev);
+		return ret;
+	}
+
+	return 0;
+}
+
+static int fsl_edma3_remove(struct platform_device *pdev)
+{
+	struct device_node *np = pdev->dev.of_node;
+	struct fsl_edma3_engine *fsl_edma3 = platform_get_drvdata(pdev);
+
+	of_dma_controller_free(np);
+	dma_async_device_unregister(&fsl_edma3->dma_dev);
+
+	clk_disable_unprepare(fsl_edma3->clk_mp);
+
+	return 0;
+}
+
+#ifdef CONFIG_PM_SLEEP
+static int fsl_edma3_suspend_late(struct device *dev)
+{
+	struct fsl_edma3_engine *fsl_edma = dev_get_drvdata(dev);
+	struct fsl_edma3_chan *fsl_chan;
+	unsigned long flags;
+	void __iomem *addr;
+	int i;
+
+	for (i = 0; i < fsl_edma->n_chans; i++) {
+		fsl_chan = &fsl_edma->chans[i];
+		addr = fsl_chan->membase;
+
+		if ((fsl_chan->edma3->drvdata->has_pd &&
+		    pm_runtime_status_suspended(fsl_chan->dev)) ||
+		    (!fsl_chan->edma3->drvdata->has_pd && !fsl_chan->srcid))
+			continue;
+
+		if (fsl_chan->edma3->drvdata->has_pd)
+			pm_runtime_get_sync(fsl_chan->dev);
+
+		spin_lock_irqsave(&fsl_chan->vchan.lock, flags);
+		fsl_edma->edma_regs[i].csr = readl(addr + EDMA_CH_CSR);
+		fsl_edma->edma_regs[i].sbr = readl(addr + EDMA_CH_SBR);
+		/* Make sure chan is idle or will force disable. */
+		if (unlikely(!fsl_chan->idle)) {
+			dev_warn(dev, "WARN: There is non-idle channel.");
+			fsl_edma3_disable_request(fsl_chan);
+		}
+		spin_unlock_irqrestore(&fsl_chan->vchan.lock, flags);
+
+		if (fsl_chan->edma3->drvdata->has_pd)
+			pm_runtime_put_sync_suspend(fsl_chan->dev);
+	}
+
+	return 0;
+}
+
+static int fsl_edma3_resume_early(struct device *dev)
+{
+	struct fsl_edma3_engine *fsl_edma = dev_get_drvdata(dev);
+	struct fsl_edma3_chan *fsl_chan;
+	void __iomem *addr;
+	unsigned long flags;
+	int i;
+
+	for (i = 0; i < fsl_edma->n_chans; i++) {
+		fsl_chan = &fsl_edma->chans[i];
+		addr = fsl_chan->membase;
+
+		if ((fsl_chan->edma3->drvdata->has_pd &&
+		    pm_runtime_status_suspended(fsl_chan->dev)) ||
+		    (!fsl_chan->edma3->drvdata->has_pd && !fsl_chan->srcid))
+			continue;
+
+		spin_lock_irqsave(&fsl_chan->vchan.lock, flags);
+		writel(fsl_edma->edma_regs[i].csr, addr + EDMA_CH_CSR);
+		writel(fsl_edma->edma_regs[i].sbr, addr + EDMA_CH_SBR);
+		/* restore tcd if this channel not terminated before suspend */
+		if (fsl_chan->edesc)
+			fsl_edma3_set_tcd_regs(fsl_chan,
+						fsl_chan->edesc->tcd[0].vtcd);
+		spin_unlock_irqrestore(&fsl_chan->vchan.lock, flags);
+	}
+
+	return 0;
+}
+#endif
+
+static const struct dev_pm_ops fsl_edma3_pm_ops = {
+	SET_LATE_SYSTEM_SLEEP_PM_OPS(fsl_edma3_suspend_late,
+				     fsl_edma3_resume_early)
+};
+
+static struct platform_driver fsl_edma3_driver = {
+	.driver		= {
+		.name	= "fsl-edma-v3",
+		.of_match_table = fsl_edma3_dt_ids,
+		.pm     = &fsl_edma3_pm_ops,
+	},
+	.probe          = fsl_edma3_probe,
+	.remove		= fsl_edma3_remove,
+};
+
+static int __init fsl_edma3_init(void)
+{
+	return platform_driver_register(&fsl_edma3_driver);
+}
+fs_initcall(fsl_edma3_init);
+
+static void __exit fsl_edma3_exit(void)
+{
+	platform_driver_unregister(&fsl_edma3_driver);
+}
+module_exit(fsl_edma3_exit);
+
+MODULE_ALIAS("platform:fsl-edma3");
+MODULE_DESCRIPTION("Freescale eDMA-V3 engine driver");
+MODULE_LICENSE("GPL v2");
diff --git a/drivers/dma/fsl-edma.c b/drivers/dma/fsl-edma.c
index 90bb72af3..7ef48624f 100644
--- a/drivers/dma/fsl-edma.c
+++ b/drivers/dma/fsl-edma.c
@@ -53,6 +53,7 @@ static irqreturn_t fsl_edma_tx_handler(int irq, void *dev_id)
 			}
 
 			if (!fsl_chan->edesc->iscyclic) {
+				fsl_edma_get_realcnt(fsl_chan);
 				list_del(&fsl_chan->edesc->vdesc.node);
 				vchan_cookie_complete(&fsl_chan->edesc->vdesc);
 				fsl_chan->edesc = NULL;
@@ -138,23 +139,23 @@ fsl_edma_irq_init(struct platform_device *pdev, struct fsl_edma_engine *fsl_edma
 {
 	int ret;
 
-	fsl_edma->txirq = platform_get_irq_byname(pdev, "edma-tx");
-	if (fsl_edma->txirq < 0)
-		return fsl_edma->txirq;
+	*fsl_edma->txirqs = platform_get_irq_byname(pdev, "edma-tx");
+	if (*fsl_edma->txirqs < 0)
+		return *fsl_edma->txirqs;
 
 	fsl_edma->errirq = platform_get_irq_byname(pdev, "edma-err");
 	if (fsl_edma->errirq < 0)
 		return fsl_edma->errirq;
 
-	if (fsl_edma->txirq == fsl_edma->errirq) {
-		ret = devm_request_irq(&pdev->dev, fsl_edma->txirq,
+	if (*fsl_edma->txirqs == fsl_edma->errirq) {
+		ret = devm_request_irq(&pdev->dev, *fsl_edma->txirqs,
 				fsl_edma_irq_handler, 0, "eDMA", fsl_edma);
 		if (ret) {
 			dev_err(&pdev->dev, "Can't register eDMA IRQ.\n");
 			return ret;
 		}
 	} else {
-		ret = devm_request_irq(&pdev->dev, fsl_edma->txirq,
+		ret = devm_request_irq(&pdev->dev, *fsl_edma->txirqs,
 				fsl_edma_tx_handler, 0, "eDMA tx", fsl_edma);
 		if (ret) {
 			dev_err(&pdev->dev, "Can't register eDMA tx IRQ.\n");
@@ -177,14 +178,7 @@ fsl_edma2_irq_init(struct platform_device *pdev,
 		   struct fsl_edma_engine *fsl_edma)
 {
 	int i, ret, irq;
-	int count;
-
-	count = platform_irq_count(pdev);
-	dev_dbg(&pdev->dev, "%s Found %d interrupts\r\n", __func__, count);
-	if (count <= 2) {
-		dev_err(&pdev->dev, "Interrupts in DTS not correct.\n");
-		return -EINVAL;
-	}
+	int count = fsl_edma->drvdata->txirq_count + 1;
 	/*
 	 * 16 channel independent interrupts + 1 error interrupt on i.mx7ulp.
 	 * 2 channel share one interrupt, for example, ch0/ch16, ch1/ch17...
@@ -199,15 +193,18 @@ fsl_edma2_irq_init(struct platform_device *pdev,
 		sprintf(fsl_edma->chans[i].chan_name, "eDMA2-CH%02d", i);
 
 		/* The last IRQ is for eDMA err */
-		if (i == count - 1)
+		if (i == count - 1) {
+			fsl_edma->errirq = irq;
 			ret = devm_request_irq(&pdev->dev, irq,
 						fsl_edma_err_handler,
 						0, "eDMA2-ERR", fsl_edma);
-		else
+		} else {
+			fsl_edma->txirqs[i] = irq;
 			ret = devm_request_irq(&pdev->dev, irq,
 						fsl_edma_tx_handler, 0,
 						fsl_edma->chans[i].chan_name,
 						fsl_edma);
+		}
 		if (ret)
 			return ret;
 	}
@@ -215,15 +212,59 @@ fsl_edma2_irq_init(struct platform_device *pdev,
 	return 0;
 }
 
+static int fsl_edma_irq_init_s32(struct platform_device *pdev,
+				 struct fsl_edma_engine *fsl_edma)
+{
+	int i, ret, irq, txirq_count = fsl_edma->drvdata->txirq_count;
+	static const char * const names[] = {"edma-tx_0-15", "edma-tx_16-31",
+					     "edma-err"};
+
+	for (i = 0; i <= txirq_count; i++) {
+		irq = platform_get_irq_byname(pdev, names[i]);
+		if (irq < 0) {
+			dev_err(&pdev->dev, "Can't get %s IRQ.\n",
+				names[i]);
+			return irq;
+		}
+
+		if (i == txirq_count) {
+			fsl_edma->errirq = irq;
+			ret = devm_request_irq(&pdev->dev, irq,
+					       fsl_edma_err_handler, 0,
+					       names[i], fsl_edma);
+		} else {
+			fsl_edma->txirqs[i] = irq;
+			ret = devm_request_irq(&pdev->dev, irq,
+					       fsl_edma_tx_handler, 0,
+					       names[i], fsl_edma);
+		}
+
+		if (ret) {
+			dev_err(&pdev->dev,
+				"Can't register %s IRQ.\n",
+				names[i]);
+			return ret;
+		}
+	}
+
+	return 0;
+}
+
 static void fsl_edma_irq_exit(
 		struct platform_device *pdev, struct fsl_edma_engine *fsl_edma)
 {
-	if (fsl_edma->txirq == fsl_edma->errirq) {
-		devm_free_irq(&pdev->dev, fsl_edma->txirq, fsl_edma);
-	} else {
-		devm_free_irq(&pdev->dev, fsl_edma->txirq, fsl_edma);
-		devm_free_irq(&pdev->dev, fsl_edma->errirq, fsl_edma);
+	int i;
+	bool free_errirq = true;
+	u8 count = fsl_edma->drvdata->txirq_count;
+
+	for (i = 0; i < count; i++) {
+		devm_free_irq(&pdev->dev, fsl_edma->txirqs[i], fsl_edma);
+		if (fsl_edma->txirqs[i] == fsl_edma->errirq)
+			free_errirq = false;
 	}
+
+	if (free_errirq)
+		devm_free_irq(&pdev->dev, fsl_edma->errirq, fsl_edma);
 }
 
 static void fsl_disable_clocks(struct fsl_edma_engine *fsl_edma, int nr_clocks)
@@ -238,6 +279,7 @@ static struct fsl_edma_drvdata vf610_data = {
 	.version = v1,
 	.dmamuxs = DMAMUX_NR,
 	.setup_irq = fsl_edma_irq_init,
+	.txirq_count = 1,
 };
 
 static struct fsl_edma_drvdata ls1028a_data = {
@@ -245,6 +287,7 @@ static struct fsl_edma_drvdata ls1028a_data = {
 	.dmamuxs = DMAMUX_NR,
 	.mux_swap = true,
 	.setup_irq = fsl_edma_irq_init,
+	.txirq_count = 1,
 };
 
 static struct fsl_edma_drvdata imx7ulp_data = {
@@ -252,12 +295,22 @@ static struct fsl_edma_drvdata imx7ulp_data = {
 	.dmamuxs = 1,
 	.has_dmaclk = true,
 	.setup_irq = fsl_edma2_irq_init,
+	.txirq_count = 16,
+};
+
+static struct fsl_edma_drvdata s32v234_data = {
+	.version = v1,
+	.dmamuxs = DMAMUX_NR,
+	.mux_swap = true,
+	.setup_irq = fsl_edma_irq_init_s32,
+	.txirq_count = 2,
 };
 
 static const struct of_device_id fsl_edma_dt_ids[] = {
 	{ .compatible = "fsl,vf610-edma", .data = &vf610_data},
 	{ .compatible = "fsl,ls1028a-edma", .data = &ls1028a_data},
 	{ .compatible = "fsl,imx7ulp-edma", .data = &imx7ulp_data},
+	{ .compatible = "fsl,s32v234-edma", .data = &s32v234_data},
 	{ /* sentinel */ }
 };
 MODULE_DEVICE_TABLE(of, fsl_edma_dt_ids);
@@ -365,6 +418,12 @@ static int fsl_edma_probe(struct platform_device *pdev)
 	}
 
 	edma_writel(fsl_edma, ~0, regs->intl);
+
+	fsl_edma->txirqs = devm_kzalloc(&pdev->dev,
+		drvdata->txirq_count * sizeof(*fsl_edma->txirqs), GFP_KERNEL);
+	if (!fsl_edma->txirqs)
+		return -ENOMEM;
+
 	ret = fsl_edma->drvdata->setup_irq(pdev, fsl_edma);
 	if (ret)
 		return ret;
diff --git a/drivers/dma/imx-sdma.c b/drivers/dma/imx-sdma.c
index 306f93e4b..3df4e89fd 100644
--- a/drivers/dma/imx-sdma.c
+++ b/drivers/dma/imx-sdma.c
@@ -23,10 +23,12 @@
 #include <linux/semaphore.h>
 #include <linux/spinlock.h>
 #include <linux/device.h>
+#include <linux/genalloc.h>
 #include <linux/dma-mapping.h>
 #include <linux/firmware.h>
 #include <linux/slab.h>
 #include <linux/platform_device.h>
+#include <linux/pm_runtime.h>
 #include <linux/dmaengine.h>
 #include <linux/of.h>
 #include <linux/of_address.h>
@@ -74,6 +76,9 @@
 #define SDMA_CHNENBL0_IMX35	0x200
 #define SDMA_CHNENBL0_IMX31	0x080
 #define SDMA_CHNPRI_0		0x100
+#define SDMA_DONE0_CONFIG	0x1000
+#define SDMA_DONE0_CONFIG_DONE_SEL	0x7
+#define SDMA_DONE0_CONFIG_DONE_DIS	0x6
 
 /*
  * Buffer descriptor status values.
@@ -168,6 +173,8 @@
 #define SDMA_WATERMARK_LEVEL_SPDIF	BIT(10)
 #define SDMA_WATERMARK_LEVEL_SP		BIT(11)
 #define SDMA_WATERMARK_LEVEL_DP		BIT(12)
+#define SDMA_WATERMARK_LEVEL_SD		BIT(13)
+#define SDMA_WATERMARK_LEVEL_DD		BIT(14)
 #define SDMA_WATERMARK_LEVEL_HWML	(0xFF << 16)
 #define SDMA_WATERMARK_LEVEL_LWE	BIT(28)
 #define SDMA_WATERMARK_LEVEL_HWE	BIT(29)
@@ -175,12 +182,19 @@
 
 #define SDMA_DMA_BUSWIDTHS	(BIT(DMA_SLAVE_BUSWIDTH_1_BYTE) | \
 				 BIT(DMA_SLAVE_BUSWIDTH_2_BYTES) | \
+				 BIT(DMA_SLAVE_BUSWIDTH_3_BYTES) | \
 				 BIT(DMA_SLAVE_BUSWIDTH_4_BYTES))
 
 #define SDMA_DMA_DIRECTIONS	(BIT(DMA_DEV_TO_MEM) | \
 				 BIT(DMA_MEM_TO_DEV) | \
 				 BIT(DMA_DEV_TO_DEV))
 
+#define SDMA_WATERMARK_LEVEL_FIFOS_OFF	12
+#define SDMA_WATERMARK_LEVEL_FIFO_OFF_OFF 16
+#define SDMA_WATERMARK_LEVEL_SW_DONE	BIT(23)
+#define SDMA_WATERMARK_LEVEL_SW_DONE_SEL_OFF 24
+#define SDMA_WATERMARK_LEVEL_WORDS_PER_FIFO_OFF 28
+
 /*
  * Mode/Count of data node descriptors - IPCv2
  */
@@ -364,8 +378,9 @@ struct sdma_channel {
 	struct sdma_desc		*desc;
 	struct sdma_engine		*sdma;
 	unsigned int			channel;
-	enum dma_transfer_direction		direction;
+	enum dma_transfer_direction	direction;
 	struct dma_slave_config		slave_config;
+	struct sdma_audio_config	*audio_config;
 	enum sdma_peripheral_type	peripheral_type;
 	unsigned int			event_id0;
 	unsigned int			event_id1;
@@ -381,6 +396,9 @@ struct sdma_channel {
 	enum dma_status			status;
 	struct imx_dma_data		data;
 	struct work_struct		terminate_worker;
+	struct list_head                terminated;
+	bool				is_ram_script;
+	int				prio;
 };
 
 #define IMX_DMA_SG_LOOP		BIT(0)
@@ -390,6 +408,15 @@ struct sdma_channel {
 #define MXC_SDMA_MIN_PRIORITY 1
 #define MXC_SDMA_MAX_PRIORITY 7
 
+/*
+ * 0x78(SDMA_XTRIG_CONF2+4)~0x100(SDMA_CHNPRI_O) registers are reserved and
+ * can't be accessed. Skip these register touch in suspend/resume. Also below
+ * two macros are only used on i.mx6sx.
+ */
+#define MXC_SDMA_RESERVED_REG (SDMA_CHNPRI_0 - SDMA_XTRIG_CONF2 - 4)
+#define MXC_SDMA_SAVED_REG_NUM (((SDMA_CHNENBL0_IMX35 + 4 * 48) - \
+				MXC_SDMA_RESERVED_REG) / 4)
+
 #define SDMA_FIRMWARE_MAGIC 0x414d4453
 
 /**
@@ -421,15 +448,29 @@ struct sdma_driver_data {
 	int num_events;
 	struct sdma_script_start_addrs	*script_addrs;
 	bool check_ratio;
+	/*
+	 * ecspi ERR009165 fixed should be done in sdma script
+	 * and it be fixed in soc from i.mx6ul.
+	 * please get more information from below link:
+	 * https://www.nxp.com/docs/en/errata/IMX6DQCE.pdf
+	 */
+	bool ecspi_fixed;
+	bool has_done0;
+	bool pm_runtime;
 };
 
 struct sdma_engine {
 	struct device			*dev;
 	struct sdma_channel		channel[MAX_DMA_CHANNELS];
 	struct sdma_channel_control	*channel_control;
+	u32				save_regs[MXC_SDMA_SAVED_REG_NUM];
+	u32				save_done0_regs[2];
+	const char			*fw_name;
 	void __iomem			*regs;
 	struct sdma_context_data	*context;
 	dma_addr_t			context_phys;
+	dma_addr_t			ccb_phys;
+	bool				is_on;
 	struct dma_device		dma_device;
 	struct clk			*clk_ipg;
 	struct clk			*clk_ahb;
@@ -444,6 +485,11 @@ struct sdma_engine {
 	struct sdma_buffer_descriptor	*bd0;
 	/* clock ratio for AHB:SDMA core. 1:1 is 1, 2:1 is 0*/
 	bool				clk_ratio;
+	struct gen_pool			*iram_pool;
+	bool				fw_loaded;
+	u32				fw_fail;
+	u8				*fw_data;
+	unsigned short			ram_code_start;
 };
 
 static int sdma_config_write(struct dma_chan *chan,
@@ -540,6 +586,31 @@ static struct sdma_driver_data sdma_imx6q = {
 	.script_addrs = &sdma_script_imx6q,
 };
 
+static struct sdma_script_start_addrs sdma_script_imx6sx = {
+	.ap_2_ap_addr = 642,
+	.uart_2_mcu_addr = 817,
+	.mcu_2_app_addr = 747,
+	.uartsh_2_mcu_addr = 1032,
+	.mcu_2_shp_addr = 960,
+	.app_2_mcu_addr = 683,
+	.shp_2_mcu_addr = 891,
+	.spdif_2_mcu_addr = 1100,
+	.mcu_2_spdif_addr = 1134,
+};
+
+static struct sdma_driver_data sdma_imx6sx = {
+	.chnenbl0 = SDMA_CHNENBL0_IMX35,
+	.num_events = 48,
+	.script_addrs = &sdma_script_imx6sx,
+};
+
+static struct sdma_driver_data sdma_imx6ul = {
+	.chnenbl0 = SDMA_CHNENBL0_IMX35,
+	.num_events = 48,
+	.script_addrs = &sdma_script_imx6sx,
+	.ecspi_fixed = true,
+};
+
 static struct sdma_script_start_addrs sdma_script_imx7d = {
 	.ap_2_ap_addr = 644,
 	.uart_2_mcu_addr = 819,
@@ -565,6 +636,16 @@ static struct sdma_driver_data sdma_imx8mq = {
 	.check_ratio = 1,
 };
 
+static struct sdma_driver_data sdma_imx8mp = {
+	.chnenbl0 = SDMA_CHNENBL0_IMX35,
+	.num_events = 48,
+	.script_addrs = &sdma_script_imx7d,
+	.check_ratio = 1,
+	.ecspi_fixed = true,
+	.has_done0 = true,
+	.pm_runtime = true,
+};
+
 static const struct platform_device_id sdma_devtypes[] = {
 	{
 		.name = "imx25-sdma",
@@ -584,12 +665,21 @@ static const struct platform_device_id sdma_devtypes[] = {
 	}, {
 		.name = "imx6q-sdma",
 		.driver_data = (unsigned long)&sdma_imx6q,
+	}, {
+		.name = "imx6sx-sdma",
+		.driver_data = (unsigned long)&sdma_imx6sx,
 	}, {
 		.name = "imx7d-sdma",
 		.driver_data = (unsigned long)&sdma_imx7d,
+	}, {
+		.name = "imx6ul-sdma",
+		.driver_data = (unsigned long)&sdma_imx6ul,
 	}, {
 		.name = "imx8mq-sdma",
 		.driver_data = (unsigned long)&sdma_imx8mq,
+	}, {
+		.name = "imx8mp-sdma",
+		.driver_data = (unsigned long)&sdma_imx8mp,
 	}, {
 		/* sentinel */
 	}
@@ -603,8 +693,11 @@ static const struct of_device_id sdma_dt_ids[] = {
 	{ .compatible = "fsl,imx35-sdma", .data = &sdma_imx35, },
 	{ .compatible = "fsl,imx31-sdma", .data = &sdma_imx31, },
 	{ .compatible = "fsl,imx25-sdma", .data = &sdma_imx25, },
+	{ .compatible = "fsl,imx6sx-sdma", .data = &sdma_imx6sx, },
 	{ .compatible = "fsl,imx7d-sdma", .data = &sdma_imx7d, },
+	{ .compatible = "fsl,imx6ul-sdma", .data = &sdma_imx6ul, },
 	{ .compatible = "fsl,imx8mq-sdma", .data = &sdma_imx8mq, },
+	{ .compatible = "fsl,imx8mp-sdma", .data = &sdma_imx8mp, },
 	{ /* sentinel */ }
 };
 MODULE_DEVICE_TABLE(of, sdma_dt_ids);
@@ -672,7 +765,7 @@ static int sdma_run_channel0(struct sdma_engine *sdma)
 	sdma_enable_channel(sdma, 0);
 
 	ret = readl_relaxed_poll_timeout_atomic(sdma->regs + SDMA_H_STATSTOP,
-						reg, !(reg & 1), 1, 500);
+						reg, !(reg & 1), 1, 5000);
 	if (ret)
 		dev_err(sdma->dev, "Timeout waiting for CH0 ready\n");
 
@@ -686,36 +779,80 @@ static int sdma_run_channel0(struct sdma_engine *sdma)
 	return ret;
 }
 
-static int sdma_load_script(struct sdma_engine *sdma, void *buf, int size,
-		u32 address)
+#define SDMA_SCRIPT_ADDRS_ARRAY_SIZE_V1	34
+#define SDMA_SCRIPT_ADDRS_ARRAY_SIZE_V2	38
+#define SDMA_SCRIPT_ADDRS_ARRAY_SIZE_V3	47
+#define SDMA_SCRIPT_ADDRS_ARRAY_SIZE_V4	48
+
+static void sdma_add_scripts(struct sdma_engine *sdma,
+		const struct sdma_script_start_addrs *addr)
+{
+	s32 *addr_arr = (u32 *)addr;
+	s32 *saddr_arr = (u32 *)sdma->script_addrs;
+	int i;
+
+	/* use the default firmware in ROM if missing external firmware */
+	if (!sdma->script_number)
+		sdma->script_number = SDMA_SCRIPT_ADDRS_ARRAY_SIZE_V1;
+
+	if (sdma->script_number > sizeof(struct sdma_script_start_addrs)
+				  / sizeof(s32)) {
+		dev_err(sdma->dev,
+			"SDMA script number %d not match with firmware.\n",
+			sdma->script_number);
+		return;
+	}
+
+	for (i = 0; i < sdma->script_number; i++)
+		if (addr_arr[i] > 0)
+			saddr_arr[i] = addr_arr[i];
+}
+
+static int sdma_load_script(struct sdma_engine *sdma)
 {
 	struct sdma_buffer_descriptor *bd0 = sdma->bd0;
+	const struct sdma_script_start_addrs *addr;
+	struct sdma_firmware_header *header;
+	unsigned short *ram_code;
 	void *buf_virt;
 	dma_addr_t buf_phys;
 	int ret;
 	unsigned long flags;
 
-	buf_virt = dma_alloc_coherent(sdma->dev, size, &buf_phys, GFP_KERNEL);
-	if (!buf_virt) {
+	header = (struct sdma_firmware_header *)sdma->fw_data;
+
+	addr = (void *)header + header->script_addrs_start;
+	ram_code = (void *)header + header->ram_code_start;
+	sdma->ram_code_start = header->ram_code_start;
+
+	buf_virt = dma_alloc_coherent(sdma->dev, header->ram_code_size,
+				      &buf_phys, GFP_KERNEL);
+	if (!buf_virt)
 		return -ENOMEM;
-	}
 
 	spin_lock_irqsave(&sdma->channel_0_lock, flags);
 
 	bd0->mode.command = C0_SETPM;
 	bd0->mode.status = BD_DONE | BD_WRAP | BD_EXTD;
-	bd0->mode.count = size / 2;
+	bd0->mode.count = header->ram_code_size / 2;
 	bd0->buffer_addr = buf_phys;
-	bd0->ext_buffer_addr = address;
+	bd0->ext_buffer_addr = addr->ram_code_start_addr;
 
-	memcpy(buf_virt, buf, size);
+	memcpy(buf_virt, ram_code, header->ram_code_size);
 
 	ret = sdma_run_channel0(sdma);
 
 	spin_unlock_irqrestore(&sdma->channel_0_lock, flags);
 
-	dma_free_coherent(sdma->dev, size, buf_virt, buf_phys);
+	dma_free_coherent(sdma->dev, header->ram_code_size, buf_virt, buf_phys);
+
+	sdma_add_scripts(sdma, addr);
+
+	sdma->fw_loaded = true;
 
+	dev_info_once(sdma->dev, "loaded firmware %d.%d\n",
+			header->version_major,
+			header->version_minor);
 	return ret;
 }
 
@@ -729,6 +866,38 @@ static void sdma_event_enable(struct sdma_channel *sdmac, unsigned int event)
 	val = readl_relaxed(sdma->regs + chnenbl);
 	__set_bit(channel, &val);
 	writel_relaxed(val, sdma->regs + chnenbl);
+
+	/* Set SDMA_DONEx_CONFIG is sw_done enabled */
+	if (sdmac->audio_config && sdmac->audio_config->sw_done_sel & BIT(31)) {
+		u32 sw_done_sel = sdmac->audio_config->sw_done_sel & 0xff;
+		u32 offset = SDMA_DONE0_CONFIG + sw_done_sel / 4;
+		u32 done_sel = SDMA_DONE0_CONFIG_DONE_SEL +
+				((sw_done_sel % 4) << 3);
+		u32 sw_done_dis = SDMA_DONE0_CONFIG_DONE_DIS +
+				((sw_done_sel % 4) << 3);
+
+		val = readl_relaxed(sdma->regs + offset);
+		__set_bit(done_sel, &val);
+		__clear_bit(sw_done_dis, &val);
+		writel_relaxed(val, sdma->regs + offset);
+	}
+
+}
+
+static int sdma_set_channel_priority(struct sdma_channel *sdmac,
+		unsigned int priority)
+{
+	struct sdma_engine *sdma = sdmac->sdma;
+	int channel = sdmac->channel;
+
+	if (priority < MXC_SDMA_MIN_PRIORITY
+	    || priority > MXC_SDMA_MAX_PRIORITY) {
+		return -EINVAL;
+	}
+
+	writel_relaxed(priority, sdma->regs + SDMA_CHNPRI_0 + 4 * channel);
+
+	return 0;
 }
 
 static void sdma_event_disable(struct sdma_channel *sdmac, unsigned int event)
@@ -773,6 +942,7 @@ static void sdma_update_channel_loop(struct sdma_channel *sdmac)
 	struct sdma_buffer_descriptor *bd;
 	int error = 0;
 	enum dma_status	old_status = sdmac->status;
+	int count = 0;
 
 	/*
 	 * loop mode. Iterate over descriptors, re-setup them and
@@ -783,6 +953,17 @@ static void sdma_update_channel_loop(struct sdma_channel *sdmac)
 
 		bd = &desc->bd[desc->buf_tail];
 
+		/*
+		 * re-enable HSTART_HE if all bds consumed at the last time,
+		 * that happens in high loading case which sdma_handle_channel_
+		 * loop can't be handled in time while all bds run out in sdma
+		 * side, then sdma script clear HE and cause channel stop.
+		 */
+		if (count == desc->num_bd) {
+			dev_warn(sdmac->sdma->dev, "All bds consumed,restart now.\n");
+			sdma_enable_channel(sdmac->sdma, sdmac->channel);
+		}
+
 		if (bd->mode.status & BD_DONE)
 			break;
 
@@ -795,13 +976,20 @@ static void sdma_update_channel_loop(struct sdma_channel *sdmac)
 	       /*
 		* We use bd->mode.count to calculate the residue, since contains
 		* the number of bytes present in the current buffer descriptor.
+		* Note: in IMX_DMATYPE_MULTI_SAI case, bd->mode.count used as
+		* remaining bytes instead so that one register could be saved.
+		* so chn_real_count = desc->period_len - bd->mode.count.
 		*/
+		if (sdmac->peripheral_type == IMX_DMATYPE_MULTI_SAI)
+			desc->chn_real_count = desc->period_len - bd->mode.count;
+		else
+			desc->chn_real_count = bd->mode.count;
 
-		desc->chn_real_count = bd->mode.count;
 		bd->mode.status |= BD_DONE;
 		bd->mode.count = desc->period_len;
 		desc->buf_ptail = desc->buf_tail;
 		desc->buf_tail = (desc->buf_tail + 1) % desc->num_bd;
+		count++;
 
 		/*
 		 * The callback is called from the interrupt context in order
@@ -848,6 +1036,13 @@ static irqreturn_t sdma_int_handler(int irq, void *dev_id)
 	struct sdma_engine *sdma = dev_id;
 	unsigned long stat;
 
+	if (sdma->drvdata->pm_runtime)
+		pm_runtime_get_sync(sdma->dev);
+	else {
+		clk_enable(sdma->clk_ipg);
+		clk_enable(sdma->clk_ahb);
+	}
+
 	stat = readl_relaxed(sdma->regs + SDMA_H_INTR);
 	writel_relaxed(stat, sdma->regs + SDMA_H_INTR);
 	/* channel 0 is special and not handled here, see run_channel0() */
@@ -862,7 +1057,10 @@ static irqreturn_t sdma_int_handler(int irq, void *dev_id)
 		desc = sdmac->desc;
 		if (desc) {
 			if (sdmac->flags & IMX_DMA_SG_LOOP) {
-				sdma_update_channel_loop(sdmac);
+				if (sdmac->peripheral_type != IMX_DMATYPE_HDMI)
+					sdma_update_channel_loop(sdmac);
+				else
+					vchan_cyclic_callback(&desc->vd);
 			} else {
 				mxc_sdma_handle_channel_normal(sdmac);
 				vchan_cookie_complete(&desc->vd);
@@ -874,6 +1072,14 @@ static irqreturn_t sdma_int_handler(int irq, void *dev_id)
 		__clear_bit(channel, &stat);
 	}
 
+	if (sdma->drvdata->pm_runtime) {
+		pm_runtime_mark_last_busy(sdma->dev);
+		pm_runtime_put_autosuspend(sdma->dev);
+	} else {
+		clk_disable(sdma->clk_ipg);
+		clk_disable(sdma->clk_ahb);
+	}
+
 	return IRQ_HANDLED;
 }
 
@@ -921,6 +1127,10 @@ static void sdma_get_pc(struct sdma_channel *sdmac,
 		emi_2_per = sdma->script_addrs->mcu_2_ata_addr;
 		break;
 	case IMX_DMATYPE_CSPI:
+		per_2_emi = sdma->script_addrs->app_2_mcu_addr;
+		emi_2_per = sdma->script_addrs->mcu_2_ecspi_addr;
+		sdmac->is_ram_script = true;
+		break;
 	case IMX_DMATYPE_EXT:
 	case IMX_DMATYPE_SSI:
 	case IMX_DMATYPE_SAI:
@@ -930,6 +1140,7 @@ static void sdma_get_pc(struct sdma_channel *sdmac,
 	case IMX_DMATYPE_SSI_DUAL:
 		per_2_emi = sdma->script_addrs->ssish_2_mcu_addr;
 		emi_2_per = sdma->script_addrs->mcu_2_ssish_addr;
+		sdmac->is_ram_script = true;
 		break;
 	case IMX_DMATYPE_SSI_SP:
 	case IMX_DMATYPE_MMC:
@@ -944,11 +1155,13 @@ static void sdma_get_pc(struct sdma_channel *sdmac,
 		per_2_emi = sdma->script_addrs->asrc_2_mcu_addr;
 		emi_2_per = sdma->script_addrs->asrc_2_mcu_addr;
 		per_2_per = sdma->script_addrs->per_2_per_addr;
+		sdmac->is_ram_script = true;
 		break;
 	case IMX_DMATYPE_ASRC_SP:
 		per_2_emi = sdma->script_addrs->shp_2_mcu_addr;
 		emi_2_per = sdma->script_addrs->mcu_2_shp_addr;
 		per_2_per = sdma->script_addrs->per_2_per_addr;
+		sdmac->is_ram_script = true;
 		break;
 	case IMX_DMATYPE_MSHC:
 		per_2_emi = sdma->script_addrs->mshc_2_mcu_addr;
@@ -964,6 +1177,19 @@ static void sdma_get_pc(struct sdma_channel *sdmac,
 	case IMX_DMATYPE_IPU_MEMORY:
 		emi_2_per = sdma->script_addrs->ext_mem_2_ipu_addr;
 		break;
+	case IMX_DMATYPE_HDMI:
+		emi_2_per = sdma->script_addrs->hdmi_dma_addr;
+		sdmac->is_ram_script = true;
+		break;
+	case IMX_DMATYPE_MULTI_SAI:
+		per_2_emi = sdma->script_addrs->sai_2_mcu_addr;
+		emi_2_per = sdma->script_addrs->mcu_2_sai_addr;
+		sdmac->is_ram_script = true;
+		break;
+	case IMX_DMATYPE_I2C:
+		per_2_emi = sdma->script_addrs->i2c_2_mcu_addr;
+		emi_2_per = sdma->script_addrs->mcu_2_i2c_addr;
+		sdmac->is_ram_script = true;
 	default:
 		break;
 	}
@@ -1011,11 +1237,16 @@ static int sdma_load_context(struct sdma_channel *sdmac)
 	/* Send by context the event mask,base address for peripheral
 	 * and watermark level
 	 */
-	context->gReg[0] = sdmac->event_mask[1];
-	context->gReg[1] = sdmac->event_mask[0];
-	context->gReg[2] = sdmac->per_addr;
-	context->gReg[6] = sdmac->shp_addr;
-	context->gReg[7] = sdmac->watermark_level;
+	if (sdmac->peripheral_type == IMX_DMATYPE_HDMI) {
+		context->gReg[4] = sdmac->per_addr;
+		context->gReg[6] = sdmac->shp_addr;
+	} else {
+		context->gReg[0] = sdmac->event_mask[1];
+		context->gReg[1] = sdmac->event_mask[0];
+		context->gReg[2] = sdmac->per_addr;
+		context->gReg[6] = sdmac->shp_addr;
+		context->gReg[7] = sdmac->watermark_level;
+	}
 
 	bd0->mode.command = C0_SETDM;
 	bd0->mode.status = BD_DONE | BD_WRAP | BD_EXTD;
@@ -1029,6 +1260,31 @@ static int sdma_load_context(struct sdma_channel *sdmac)
 	return ret;
 }
 
+static int sdma_save_restore_context(struct sdma_engine *sdma, bool save)
+{
+	struct sdma_context_data *context = sdma->context;
+	struct sdma_buffer_descriptor *bd0 = sdma->bd0;
+	unsigned long flags;
+	int ret;
+
+	spin_lock_irqsave(&sdma->channel_0_lock, flags);
+
+	if (save)
+		bd0->mode.command = C0_GETDM;
+	else
+		bd0->mode.command = C0_SETDM;
+
+	bd0->mode.status = BD_DONE | BD_WRAP | BD_EXTD;
+	bd0->mode.count = MAX_DMA_CHANNELS * sizeof(*context) / 4;
+	bd0->buffer_addr = sdma->context_phys;
+	bd0->ext_buffer_addr = 2048;
+	ret = sdma_run_channel0(sdma);
+
+	spin_unlock_irqrestore(&sdma->channel_0_lock, flags);
+
+	return ret;
+}
+
 static struct sdma_channel *to_sdma_chan(struct dma_chan *chan)
 {
 	return container_of(chan, struct sdma_channel, vc.chan);
@@ -1049,9 +1305,6 @@ static void sdma_channel_terminate_work(struct work_struct *work)
 {
 	struct sdma_channel *sdmac = container_of(work, struct sdma_channel,
 						  terminate_worker);
-	unsigned long flags;
-	LIST_HEAD(head);
-
 	/*
 	 * According to NXP R&D team a delay of one BD SDMA cost time
 	 * (maximum is 1ms) should be added after disable of the channel
@@ -1060,10 +1313,7 @@ static void sdma_channel_terminate_work(struct work_struct *work)
 	 */
 	usleep_range(1000, 2000);
 
-	spin_lock_irqsave(&sdmac->vc.lock, flags);
-	vchan_get_all_descriptors(&sdmac->vc, &head);
-	spin_unlock_irqrestore(&sdmac->vc.lock, flags);
-	vchan_dma_desc_free_list(&sdmac->vc, &head);
+	vchan_dma_desc_free_list(&sdmac->vc, &sdmac->terminated);
 }
 
 static int sdma_terminate_all(struct dma_chan *chan)
@@ -1071,18 +1321,33 @@ static int sdma_terminate_all(struct dma_chan *chan)
 	struct sdma_channel *sdmac = to_sdma_chan(chan);
 	unsigned long flags;
 
+	if (sdmac->sdma->drvdata->pm_runtime)
+		pm_runtime_get_sync(sdmac->sdma->dev);
+
 	spin_lock_irqsave(&sdmac->vc.lock, flags);
 
 	sdma_disable_channel(chan);
 
 	if (sdmac->desc) {
 		vchan_terminate_vdesc(&sdmac->desc->vd);
+		/*
+		 * move out current descriptor into terminated list so that
+		 * it could be free in sdma_channel_terminate_work alone
+		 * later without potential involving next descriptor raised
+		 * up before the last descriptor terminated.
+		 */
+		vchan_get_all_descriptors(&sdmac->vc, &sdmac->terminated);
 		sdmac->desc = NULL;
 		schedule_work(&sdmac->terminate_worker);
 	}
 
 	spin_unlock_irqrestore(&sdmac->vc.lock, flags);
 
+	if (sdmac->sdma->drvdata->pm_runtime) {
+		pm_runtime_mark_last_busy(sdmac->sdma->dev);
+		pm_runtime_put_autosuspend(sdmac->sdma->dev);
+	}
+
 	return 0;
 }
 
@@ -1133,6 +1398,49 @@ static void sdma_set_watermarklevel_for_p2p(struct sdma_channel *sdmac)
 		sdmac->watermark_level |= SDMA_WATERMARK_LEVEL_DP;
 
 	sdmac->watermark_level |= SDMA_WATERMARK_LEVEL_CONT;
+
+	if (sdmac->audio_config->src_fifo_num > 1)
+		sdmac->watermark_level |= SDMA_WATERMARK_LEVEL_SD;
+	if (sdmac->audio_config->dst_fifo_num > 1)
+		sdmac->watermark_level |= SDMA_WATERMARK_LEVEL_DD;
+}
+
+static void sdma_set_watermarklevel_for_sais(struct sdma_channel *sdmac)
+{
+	u8 fifo_num = sdmac->audio_config->src_fifo_num |
+		      sdmac->audio_config->dst_fifo_num;
+	u8 fifo_offset = sdmac->audio_config->src_fifo_off |
+			 sdmac->audio_config->dst_fifo_off;
+	u8 words_per_fifo = sdmac->audio_config->words_per_fifo;
+
+	sdmac->watermark_level &= ~(0xFF << SDMA_WATERMARK_LEVEL_FIFOS_OFF |
+				    SDMA_WATERMARK_LEVEL_SW_DONE |
+				    0xf << SDMA_WATERMARK_LEVEL_SW_DONE_SEL_OFF |
+				    0xf << SDMA_WATERMARK_LEVEL_FIFO_OFF_OFF |
+				    0xf << SDMA_WATERMARK_LEVEL_WORDS_PER_FIFO_OFF);
+
+	if (sdmac->audio_config->sw_done_sel & BIT(31))
+		sdmac->watermark_level |= SDMA_WATERMARK_LEVEL_SW_DONE |
+				(sdmac->audio_config->sw_done_sel & 0xff) <<
+				SDMA_WATERMARK_LEVEL_SW_DONE_SEL_OFF;
+
+	/* For fifo_num
+	 * bit 12-15 is the fifo number;
+	 * bit 16-19 is the fifo offset,
+	 * bit 28-31 is the channels per fifo.
+	 * so here only need to shift left fifo_num 12 bit for watermake_level
+	 */
+	if (fifo_num)
+		sdmac->watermark_level |= fifo_num <<
+					SDMA_WATERMARK_LEVEL_FIFOS_OFF;
+
+	if (fifo_offset)
+		sdmac->watermark_level |= fifo_offset <<
+					SDMA_WATERMARK_LEVEL_FIFO_OFF_OFF;
+
+	if (words_per_fifo)
+		sdmac->watermark_level |= (words_per_fifo - 1) <<
+					SDMA_WATERMARK_LEVEL_WORDS_PER_FIFO_OFF;
 }
 
 static int sdma_config_channel(struct dma_chan *chan)
@@ -1158,6 +1466,12 @@ static int sdma_config_channel(struct dma_chan *chan)
 		break;
 	}
 
+	sdma_set_channel_priority(sdmac, sdmac->prio);
+
+	sdma_event_enable(sdmac, sdmac->event_id0);
+	if (sdmac->event_id1)
+		sdma_event_enable(sdmac, sdmac->event_id1);
+
 	sdma_get_pc(sdmac, sdmac->peripheral_type);
 
 	if ((sdmac->peripheral_type != IMX_DMATYPE_MEMORY) &&
@@ -1167,8 +1481,21 @@ static int sdma_config_channel(struct dma_chan *chan)
 			if (sdmac->peripheral_type == IMX_DMATYPE_ASRC_SP ||
 			    sdmac->peripheral_type == IMX_DMATYPE_ASRC)
 				sdma_set_watermarklevel_for_p2p(sdmac);
-		} else
+		} else {
+			/*
+			 * ERR009165 fixed from i.mx6ul, no errata need,
+			 * set bit31 to let sdma script skip the errata.
+			 */
+			if (sdmac->peripheral_type == IMX_DMATYPE_CSPI &&
+			    sdmac->direction == DMA_MEM_TO_DEV &&
+			    sdmac->sdma->drvdata->ecspi_fixed)
+				__set_bit(31, &sdmac->watermark_level);
+			else if (sdmac->peripheral_type ==
+					IMX_DMATYPE_MULTI_SAI)
+				sdma_set_watermarklevel_for_sais(sdmac);
+
 			__set_bit(sdmac->event_id0, sdmac->event_mask);
+		}
 
 		/* Address */
 		sdmac->shp_addr = sdmac->per_address;
@@ -1180,28 +1507,18 @@ static int sdma_config_channel(struct dma_chan *chan)
 	return 0;
 }
 
-static int sdma_set_channel_priority(struct sdma_channel *sdmac,
-		unsigned int priority)
-{
-	struct sdma_engine *sdma = sdmac->sdma;
-	int channel = sdmac->channel;
-
-	if (priority < MXC_SDMA_MIN_PRIORITY
-	    || priority > MXC_SDMA_MAX_PRIORITY) {
-		return -EINVAL;
-	}
-
-	writel_relaxed(priority, sdma->regs + SDMA_CHNPRI_0 + 4 * channel);
-
-	return 0;
-}
-
 static int sdma_request_channel0(struct sdma_engine *sdma)
 {
 	int ret = -EBUSY;
 
-	sdma->bd0 = dma_alloc_coherent(sdma->dev, PAGE_SIZE, &sdma->bd0_phys,
-					GFP_NOWAIT);
+	if (sdma->iram_pool)
+		sdma->bd0 = gen_pool_dma_alloc(sdma->iram_pool,
+					sizeof(struct sdma_buffer_descriptor),
+					&sdma->bd0_phys);
+	else
+		sdma->bd0 = dma_alloc_coherent(sdma->dev,
+					sizeof(struct sdma_buffer_descriptor),
+					&sdma->bd0_phys, GFP_NOWAIT);
 	if (!sdma->bd0) {
 		ret = -ENOMEM;
 		goto out;
@@ -1221,10 +1538,15 @@ static int sdma_request_channel0(struct sdma_engine *sdma)
 static int sdma_alloc_bd(struct sdma_desc *desc)
 {
 	u32 bd_size = desc->num_bd * sizeof(struct sdma_buffer_descriptor);
+	struct sdma_engine *sdma = desc->sdmac->sdma;
 	int ret = 0;
 
-	desc->bd = dma_alloc_coherent(desc->sdmac->sdma->dev, bd_size,
-				       &desc->bd_phys, GFP_NOWAIT);
+	if (sdma->iram_pool)
+		desc->bd = gen_pool_dma_alloc(sdma->iram_pool, bd_size,
+					      &desc->bd_phys);
+	else
+		desc->bd = dma_alloc_coherent(sdma->dev, bd_size,
+					      &desc->bd_phys, GFP_NOWAIT);
 	if (!desc->bd) {
 		ret = -ENOMEM;
 		goto out;
@@ -1236,9 +1558,14 @@ static int sdma_alloc_bd(struct sdma_desc *desc)
 static void sdma_free_bd(struct sdma_desc *desc)
 {
 	u32 bd_size = desc->num_bd * sizeof(struct sdma_buffer_descriptor);
+	struct sdma_engine *sdma = desc->sdmac->sdma;
 
-	dma_free_coherent(desc->sdmac->sdma->dev, bd_size, desc->bd,
-			  desc->bd_phys);
+	if (sdma->iram_pool)
+		gen_pool_free(sdma->iram_pool, (unsigned long)desc->bd,
+			      bd_size);
+	else
+		dma_free_coherent(desc->sdmac->sdma->dev, bd_size, desc->bd,
+				  desc->bd_phys);
 }
 
 static void sdma_desc_free(struct virt_dma_desc *vd)
@@ -1249,12 +1576,122 @@ static void sdma_desc_free(struct virt_dma_desc *vd)
 	kfree(desc);
 }
 
+static int sdma_runtime_suspend(struct device *dev)
+{
+	struct platform_device *pdev = to_platform_device(dev);
+	struct sdma_engine *sdma = platform_get_drvdata(pdev);
+
+	if (!sdma->is_on)
+		return 0;
+
+	sdma->fw_loaded = false;
+	sdma->is_on = false;
+
+	clk_disable(sdma->clk_ipg);
+	clk_disable(sdma->clk_ahb);
+
+	/* free channel0 bd */
+	if (sdma->iram_pool)
+		gen_pool_free(sdma->iram_pool, (unsigned long)sdma->bd0,
+			      sizeof(struct sdma_buffer_descriptor));
+	else
+		dma_free_coherent(sdma->dev,
+				  sizeof(struct sdma_buffer_descriptor),
+				  sdma->bd0, sdma->bd0_phys);
+
+	return 0;
+}
+
+static int sdma_runtime_resume(struct device *dev)
+{
+	struct platform_device *pdev = to_platform_device(dev);
+	struct sdma_engine *sdma = platform_get_drvdata(pdev);
+	int i, ret = 0;
+
+	ret = clk_enable(sdma->clk_ipg);
+	if (ret)
+		return ret;
+	ret = clk_enable(sdma->clk_ahb);
+	if (ret)
+		goto disable_clk_ipg;
+
+	/* Do nothing at HW level if audiomix which shared with audio driver
+	 * not off indeed.
+	 */
+	if (readl_relaxed(sdma->regs + SDMA_H_C0PTR)) {
+		if (sdma->iram_pool)
+			sdma->bd0 = gen_pool_dma_alloc(sdma->iram_pool,
+					sizeof(struct sdma_buffer_descriptor),
+					&sdma->bd0_phys);
+		else
+			sdma->bd0 = dma_alloc_coherent(sdma->dev,
+					sizeof(struct sdma_buffer_descriptor),
+					&sdma->bd0_phys, GFP_NOWAIT);
+		if (!sdma->bd0)
+			ret = -ENOMEM;
+
+		sdma->channel_control[0].base_bd_ptr = sdma->bd0_phys;
+		sdma->channel_control[0].current_bd_ptr = sdma->bd0_phys;
+
+		sdma->is_on = true;
+		sdma->fw_loaded = true;
+
+		return ret;
+	}
+
+	/* Be sure SDMA has not started yet */
+	writel_relaxed(0, sdma->regs + SDMA_H_C0PTR);
+
+	/* disable all channels */
+	for (i = 0; i < sdma->drvdata->num_events; i++)
+		writel_relaxed(0, sdma->regs + chnenbl_ofs(sdma, i));
+
+	/* All channels have priority 0 */
+	for (i = 0; i < MAX_DMA_CHANNELS; i++)
+		writel_relaxed(0, sdma->regs + SDMA_CHNPRI_0 + i * 4);
+
+	ret = sdma_request_channel0(sdma);
+	if (ret)
+		return ret;
+
+	sdma_config_ownership(&sdma->channel[0], false, true, false);
+
+	/* Set Command Channel (Channel Zero) */
+	writel_relaxed(0x4050, sdma->regs + SDMA_CHN0ADDR);
+
+	/* Set bits of CONFIG register but with static context switching */
+	if (sdma->clk_ratio)
+		writel_relaxed(SDMA_H_CONFIG_ACR, sdma->regs + SDMA_H_CONFIG);
+	else
+		writel_relaxed(0, sdma->regs + SDMA_H_CONFIG);
+
+	writel_relaxed(sdma->ccb_phys, sdma->regs + SDMA_H_C0PTR);
+
+	/* Initializes channel's priorities */
+	sdma_set_channel_priority(&sdma->channel[0], 7);
+
+	if (!sdma->fw_data)
+		dev_dbg(sdma->dev, "firmware not ready.\n");
+	else if (sdma_load_script(sdma))
+		dev_warn(sdma->dev, "failed to load script.\n");
+
+	sdma->is_on = true;
+
+	return 0;
+
+disable_clk_ipg:
+	clk_disable(sdma->clk_ipg);
+	dev_err(sdma->dev, "initialisation failed with %d\n", ret);
+
+	return ret;
+}
+
 static int sdma_alloc_chan_resources(struct dma_chan *chan)
 {
 	struct sdma_channel *sdmac = to_sdma_chan(chan);
 	struct imx_dma_data *data = chan->private;
 	struct imx_dma_data mem_data;
-	int prio, ret;
+	int prio;
 
 	/*
 	 * MEMCPY may never setup chan->private by filter function such as
@@ -1292,31 +1729,31 @@ static int sdma_alloc_chan_resources(struct dma_chan *chan)
 	sdmac->peripheral_type = data->peripheral_type;
 	sdmac->event_id0 = data->dma_request;
 	sdmac->event_id1 = data->dma_request2;
-
-	ret = clk_enable(sdmac->sdma->clk_ipg);
-	if (ret)
-		return ret;
-	ret = clk_enable(sdmac->sdma->clk_ahb);
-	if (ret)
-		goto disable_clk_ipg;
-
-	ret = sdma_set_channel_priority(sdmac, prio);
-	if (ret)
-		goto disable_clk_ahb;
+	sdmac->prio = prio;
 
 	return 0;
-
-disable_clk_ahb:
-	clk_disable(sdmac->sdma->clk_ahb);
-disable_clk_ipg:
-	clk_disable(sdmac->sdma->clk_ipg);
-	return ret;
 }
 
 static void sdma_free_chan_resources(struct dma_chan *chan)
 {
 	struct sdma_channel *sdmac = to_sdma_chan(chan);
-	struct sdma_engine *sdma = sdmac->sdma;
+
+	/*
+	 * Per fw_data is null which means firmware not loaded and sdma
+	 * not initialized, directly return. This happens in below case:
+	 *
+	 * -- driver request dma chan in probe phase, after that driver
+	 *    fall into -EPROBE_DEFER and free channel again without
+	 *    anything else about dma, so just return directly, otherwise
+	 *    kernel could hang since dma hardware not ready if drvdata->
+	 *    pm_runtime is false.
+	 *
+	 */
+	if (unlikely(!sdmac->sdma->fw_data))
+		return;
+
+	if (sdmac->sdma->drvdata->pm_runtime)
+		pm_runtime_get_sync(sdmac->sdma->dev);
 
 	sdma_terminate_all(chan);
 
@@ -1331,8 +1768,13 @@ static void sdma_free_chan_resources(struct dma_chan *chan)
 
 	sdma_set_channel_priority(sdmac, 0);
 
-	clk_disable(sdma->clk_ipg);
-	clk_disable(sdma->clk_ahb);
+	kfree(sdmac->audio_config);
+	sdmac->audio_config = NULL;
+
+	if (sdmac->sdma->drvdata->pm_runtime) {
+		pm_runtime_mark_last_busy(sdmac->sdma->dev);
+		pm_runtime_put_autosuspend(sdmac->sdma->dev);
+	}
 }
 
 static struct sdma_desc *sdma_transfer_init(struct sdma_channel *sdmac,
@@ -1340,6 +1782,11 @@ static struct sdma_desc *sdma_transfer_init(struct sdma_channel *sdmac,
 {
 	struct sdma_desc *desc;
 
+	if (!sdmac->sdma->fw_loaded && sdmac->is_ram_script) {
+		dev_err(sdmac->sdma->dev, "sdma firmware not ready!\n");
+		goto err_out;
+	}
+
 	desc = kzalloc((sizeof(*desc)), GFP_NOWAIT);
 	if (!desc)
 		goto err_out;
@@ -1355,7 +1802,7 @@ static struct sdma_desc *sdma_transfer_init(struct sdma_channel *sdmac,
 	desc->sdmac = sdmac;
 	desc->num_bd = bds;
 
-	if (sdma_alloc_bd(desc))
+	if (bds && sdma_alloc_bd(desc))
 		goto err_desc_out;
 
 	/* No slave_config called in MEMCPY case, so do here */
@@ -1391,10 +1838,16 @@ static struct dma_async_tx_descriptor *sdma_prep_memcpy(
 	dev_dbg(sdma->dev, "memcpy: %pad->%pad, len=%zu, channel=%d.\n",
 		&dma_src, &dma_dst, len, channel);
 
+	if (sdma->drvdata->pm_runtime)
+		pm_runtime_get_sync(sdmac->sdma->dev);
+
 	desc = sdma_transfer_init(sdmac, DMA_MEM_TO_MEM,
 					len / SDMA_BD_MAX_CNT + 1);
-	if (!desc)
+	if (!desc) {
+		if (sdma->drvdata->pm_runtime)
+			pm_runtime_put_sync_suspend(sdmac->sdma->dev);
 		return NULL;
+	}
 
 	do {
 		count = min_t(size_t, len, SDMA_BD_MAX_CNT);
@@ -1426,6 +1879,11 @@ static struct dma_async_tx_descriptor *sdma_prep_memcpy(
 		bd->mode.status = param;
 	} while (len);
 
+	if (sdmac->sdma->drvdata->pm_runtime) {
+		pm_runtime_mark_last_busy(sdmac->sdma->dev);
+		pm_runtime_put_autosuspend(sdmac->sdma->dev);
+	}
+
 	return vchan_tx_prep(&sdmac->vc, &desc->vd, flags);
 }
 
@@ -1441,6 +1899,9 @@ static struct dma_async_tx_descriptor *sdma_prep_slave_sg(
 	struct scatterlist *sg;
 	struct sdma_desc *desc;
 
+	if (sdma->drvdata->pm_runtime)
+		pm_runtime_get_sync(sdmac->sdma->dev);
+
 	sdma_config_write(chan, &sdmac->slave_config, direction);
 
 	desc = sdma_transfer_init(sdmac, direction, sg_len);
@@ -1476,6 +1937,9 @@ static struct dma_async_tx_descriptor *sdma_prep_slave_sg(
 			if (count & 3 || sg->dma_address & 3)
 				goto err_bd_out;
 			break;
+		case DMA_SLAVE_BUSWIDTH_3_BYTES:
+			bd->mode.command = 3;
+			break;
 		case DMA_SLAVE_BUSWIDTH_2_BYTES:
 			bd->mode.command = 2;
 			if (count & 1 || sg->dma_address & 1)
@@ -1504,12 +1968,19 @@ static struct dma_async_tx_descriptor *sdma_prep_slave_sg(
 		bd->mode.status = param;
 	}
 
+	if (sdmac->sdma->drvdata->pm_runtime) {
+		pm_runtime_mark_last_busy(sdmac->sdma->dev);
+		pm_runtime_put_autosuspend(sdmac->sdma->dev);
+	}
+
 	return vchan_tx_prep(&sdmac->vc, &desc->vd, flags);
 err_bd_out:
 	sdma_free_bd(desc);
 	kfree(desc);
 err_out:
 	sdmac->status = DMA_ERROR;
+	if (sdma->drvdata->pm_runtime)
+		pm_runtime_put_sync_suspend(sdmac->sdma->dev);
 	return NULL;
 }
 
@@ -1520,13 +1991,19 @@ static struct dma_async_tx_descriptor *sdma_prep_dma_cyclic(
 {
 	struct sdma_channel *sdmac = to_sdma_chan(chan);
 	struct sdma_engine *sdma = sdmac->sdma;
-	int num_periods = buf_len / period_len;
+	int num_periods = 0;
 	int channel = sdmac->channel;
 	int i = 0, buf = 0;
 	struct sdma_desc *desc;
 
 	dev_dbg(sdma->dev, "%s channel: %d\n", __func__, channel);
 
+	if (sdma->drvdata->pm_runtime)
+		pm_runtime_get_sync(sdmac->sdma->dev);
+
+	if (sdmac->peripheral_type != IMX_DMATYPE_HDMI)
+		num_periods = buf_len / period_len;
+
 	sdma_config_write(chan, &sdmac->slave_config, direction);
 
 	desc = sdma_transfer_init(sdmac, direction, num_periods);
@@ -1543,6 +2020,9 @@ static struct dma_async_tx_descriptor *sdma_prep_dma_cyclic(
 		goto err_bd_out;
 	}
 
+	if (sdmac->peripheral_type == IMX_DMATYPE_HDMI)
+		return vchan_tx_prep(&sdmac->vc, &desc->vd, flags);
+
 	while (buf < buf_len) {
 		struct sdma_buffer_descriptor *bd = &desc->bd[i];
 		int param;
@@ -1575,12 +2055,19 @@ static struct dma_async_tx_descriptor *sdma_prep_dma_cyclic(
 		i++;
 	}
 
+	if (sdmac->sdma->drvdata->pm_runtime) {
+		pm_runtime_mark_last_busy(sdmac->sdma->dev);
+		pm_runtime_put_autosuspend(sdmac->sdma->dev);
+	}
+
 	return vchan_tx_prep(&sdmac->vc, &desc->vd, flags);
 err_bd_out:
 	sdma_free_bd(desc);
 	kfree(desc);
 err_out:
 	sdmac->status = DMA_ERROR;
+	if (sdma->drvdata->pm_runtime)
+		pm_runtime_put_sync_suspend(sdmac->sdma->dev);
 	return NULL;
 }
 
@@ -1590,6 +2077,9 @@ static int sdma_config_write(struct dma_chan *chan,
 {
 	struct sdma_channel *sdmac = to_sdma_chan(chan);
 
+	sdmac->watermark_level = 0;
+	sdmac->is_ram_script = false;
+
 	if (direction == DMA_DEV_TO_MEM) {
 		sdmac->per_address = dmaengine_cfg->src_addr;
 		sdmac->watermark_level = dmaengine_cfg->src_maxburst *
@@ -1603,6 +2093,10 @@ static int sdma_config_write(struct dma_chan *chan,
 		sdmac->watermark_level |= (dmaengine_cfg->dst_maxburst << 16) &
 			SDMA_WATERMARK_LEVEL_HWML;
 		sdmac->word_size = dmaengine_cfg->dst_addr_width;
+	} else if (sdmac->peripheral_type == IMX_DMATYPE_HDMI) {
+			sdmac->per_address = dmaengine_cfg->dst_addr;
+			sdmac->per_address2 = dmaengine_cfg->src_addr;
+			sdmac->watermark_level = 0;
 	} else {
 		sdmac->per_address = dmaengine_cfg->dst_addr;
 		sdmac->watermark_level = dmaengine_cfg->dst_maxburst *
@@ -1617,19 +2111,31 @@ static int sdma_config(struct dma_chan *chan,
 		       struct dma_slave_config *dmaengine_cfg)
 {
 	struct sdma_channel *sdmac = to_sdma_chan(chan);
+	void *tmp;
 
 	memcpy(&sdmac->slave_config, dmaengine_cfg, sizeof(*dmaengine_cfg));
 
+	/* Allocate special sdma_audio_config if it's used */
+	if (dmaengine_cfg->peripheral_config) {
+		tmp = krealloc(sdmac->audio_config,
+			       dmaengine_cfg->peripheral_size, GFP_NOWAIT);
+		if (!tmp)
+			return -ENOMEM;
+
+		sdmac->audio_config = (struct sdma_audio_config *)tmp;
+
+		memcpy(tmp, dmaengine_cfg->peripheral_config,
+			dmaengine_cfg->peripheral_size);
+	}
+
 	/* Set ENBLn earlier to make sure dma request triggered after that */
 	if (sdmac->event_id0 >= sdmac->sdma->drvdata->num_events)
 		return -EINVAL;
-	sdma_event_enable(sdmac, sdmac->event_id0);
 
-	if (sdmac->event_id1) {
-		if (sdmac->event_id1 >= sdmac->sdma->drvdata->num_events)
-			return -EINVAL;
-		sdma_event_enable(sdmac, sdmac->event_id1);
-	}
+
+	if (sdmac->event_id1 &&
+	    sdmac->event_id1 >= sdmac->sdma->drvdata->num_events)
+		return -EINVAL;
 
 	return 0;
 }
@@ -1680,55 +2186,41 @@ static void sdma_issue_pending(struct dma_chan *chan)
 	struct sdma_channel *sdmac = to_sdma_chan(chan);
 	unsigned long flags;
 
+	if (sdmac->sdma->drvdata->pm_runtime)
+		pm_runtime_get_sync(sdmac->sdma->dev);
+
 	spin_lock_irqsave(&sdmac->vc.lock, flags);
 	if (vchan_issue_pending(&sdmac->vc) && !sdmac->desc)
 		sdma_start_desc(sdmac);
 	spin_unlock_irqrestore(&sdmac->vc.lock, flags);
-}
-
-#define SDMA_SCRIPT_ADDRS_ARRAY_SIZE_V1	34
-#define SDMA_SCRIPT_ADDRS_ARRAY_SIZE_V2	38
-#define SDMA_SCRIPT_ADDRS_ARRAY_SIZE_V3	41
-#define SDMA_SCRIPT_ADDRS_ARRAY_SIZE_V4	42
-
-static void sdma_add_scripts(struct sdma_engine *sdma,
-		const struct sdma_script_start_addrs *addr)
-{
-	s32 *addr_arr = (u32 *)addr;
-	s32 *saddr_arr = (u32 *)sdma->script_addrs;
-	int i;
 
-	/* use the default firmware in ROM if missing external firmware */
-	if (!sdma->script_number)
-		sdma->script_number = SDMA_SCRIPT_ADDRS_ARRAY_SIZE_V1;
-
-	if (sdma->script_number > sizeof(struct sdma_script_start_addrs)
-				  / sizeof(s32)) {
-		dev_err(sdma->dev,
-			"SDMA script number %d not match with firmware.\n",
-			sdma->script_number);
-		return;
+	if (sdmac->sdma->drvdata->pm_runtime) {
+		pm_runtime_mark_last_busy(sdmac->sdma->dev);
+		pm_runtime_put_autosuspend(sdmac->sdma->dev);
 	}
-
-	for (i = 0; i < sdma->script_number; i++)
-		if (addr_arr[i] > 0)
-			saddr_arr[i] = addr_arr[i];
 }
 
 static void sdma_load_firmware(const struct firmware *fw, void *context)
 {
 	struct sdma_engine *sdma = context;
 	const struct sdma_firmware_header *header;
-	const struct sdma_script_start_addrs *addr;
-	unsigned short *ram_code;
 
 	if (!fw) {
-		dev_info(sdma->dev, "external firmware not found, using ROM firmware\n");
-		/* In this case we just use the ROM firmware. */
+		/* Load firmware once more time if timeout */
+		if (sdma->fw_fail)
+			dev_info(sdma->dev, "external firmware not found, using ROM firmware\n");
+		else {
+			request_firmware_nowait(THIS_MODULE,
+					FW_ACTION_HOTPLUG, sdma->fw_name,
+					sdma->dev, GFP_KERNEL, sdma,
+					sdma_load_firmware);
+			sdma->fw_fail++;
+		}
+
 		return;
 	}
 
-	if (fw->size < sizeof(*header))
+	if (fw->size < sizeof(*header) || sdma->fw_loaded)
 		goto err_firmware;
 
 	header = (struct sdma_firmware_header *)fw->data;
@@ -1755,23 +2247,18 @@ static void sdma_load_firmware(const struct firmware *fw, void *context)
 		goto err_firmware;
 	}
 
-	addr = (void *)header + header->script_addrs_start;
-	ram_code = (void *)header + header->ram_code_start;
+	dev_info(sdma->dev, "firmware found.\n");
 
-	clk_enable(sdma->clk_ipg);
-	clk_enable(sdma->clk_ahb);
-	/* download the RAM image for SDMA */
-	sdma_load_script(sdma, ram_code,
-			header->ram_code_size,
-			addr->ram_code_start_addr);
-	clk_disable(sdma->clk_ipg);
-	clk_disable(sdma->clk_ahb);
+	if (!sdma->fw_data) {
+		sdma->fw_data = kmalloc(fw->size, GFP_KERNEL);
+		if (!sdma->fw_data)
+			goto err_firmware;
 
-	sdma_add_scripts(sdma, addr);
+		memcpy(sdma->fw_data, fw->data, fw->size);
 
-	dev_info(sdma->dev, "loaded firmware %d.%d\n",
-			header->version_major,
-			header->version_minor);
+		if (!sdma->drvdata->pm_runtime)
+			pm_runtime_get_sync(sdma->dev);
+	}
 
 err_firmware:
 	release_firmware(fw);
@@ -1855,79 +2342,34 @@ static int sdma_get_firmware(struct sdma_engine *sdma,
 	return ret;
 }
 
-static int sdma_init(struct sdma_engine *sdma)
+static int sdma_init_sw(struct sdma_engine *sdma)
 {
-	int i, ret;
-	dma_addr_t ccb_phys;
-
-	ret = clk_enable(sdma->clk_ipg);
-	if (ret)
-		return ret;
-	ret = clk_enable(sdma->clk_ahb);
-	if (ret)
-		goto disable_clk_ipg;
+	int ret, ccbsize;
 
 	if (sdma->drvdata->check_ratio &&
 	    (clk_get_rate(sdma->clk_ahb) == clk_get_rate(sdma->clk_ipg)))
 		sdma->clk_ratio = 1;
 
-	/* Be sure SDMA has not started yet */
-	writel_relaxed(0, sdma->regs + SDMA_H_C0PTR);
-
-	sdma->channel_control = dma_alloc_coherent(sdma->dev,
-			MAX_DMA_CHANNELS * sizeof (struct sdma_channel_control) +
-			sizeof(struct sdma_context_data),
-			&ccb_phys, GFP_KERNEL);
+	ccbsize = MAX_DMA_CHANNELS * (sizeof(struct sdma_channel_control)
+		+ sizeof(struct sdma_context_data));
 
+	if (sdma->iram_pool)
+		sdma->channel_control = gen_pool_dma_alloc(sdma->iram_pool,
+							   ccbsize, &sdma->ccb_phys);
+	else
+		sdma->channel_control = dma_alloc_coherent(sdma->dev, ccbsize,
+						&sdma->ccb_phys, GFP_KERNEL);
 	if (!sdma->channel_control) {
 		ret = -ENOMEM;
-		goto err_dma_alloc;
+		return ret;
 	}
 
 	sdma->context = (void *)sdma->channel_control +
 		MAX_DMA_CHANNELS * sizeof (struct sdma_channel_control);
-	sdma->context_phys = ccb_phys +
+	sdma->context_phys = sdma->ccb_phys +
 		MAX_DMA_CHANNELS * sizeof (struct sdma_channel_control);
 
-	/* disable all channels */
-	for (i = 0; i < sdma->drvdata->num_events; i++)
-		writel_relaxed(0, sdma->regs + chnenbl_ofs(sdma, i));
-
-	/* All channels have priority 0 */
-	for (i = 0; i < MAX_DMA_CHANNELS; i++)
-		writel_relaxed(0, sdma->regs + SDMA_CHNPRI_0 + i * 4);
-
-	ret = sdma_request_channel0(sdma);
-	if (ret)
-		goto err_dma_alloc;
-
-	sdma_config_ownership(&sdma->channel[0], false, true, false);
-
-	/* Set Command Channel (Channel Zero) */
-	writel_relaxed(0x4050, sdma->regs + SDMA_CHN0ADDR);
-
-	/* Set bits of CONFIG register but with static context switching */
-	if (sdma->clk_ratio)
-		writel_relaxed(SDMA_H_CONFIG_ACR, sdma->regs + SDMA_H_CONFIG);
-	else
-		writel_relaxed(0, sdma->regs + SDMA_H_CONFIG);
-
-	writel_relaxed(ccb_phys, sdma->regs + SDMA_H_C0PTR);
-
-	/* Initializes channel's priorities */
-	sdma_set_channel_priority(&sdma->channel[0], 7);
-
-	clk_disable(sdma->clk_ipg);
-	clk_disable(sdma->clk_ahb);
-
 	return 0;
-
-err_dma_alloc:
-	clk_disable(sdma->clk_ahb);
-disable_clk_ipg:
-	clk_disable(sdma->clk_ipg);
-	dev_err(sdma->dev, "initialisation failed with %d\n", ret);
-	return ret;
 }
 
 static bool sdma_filter_fn(struct dma_chan *chan, void *fn_param)
@@ -1954,9 +2396,11 @@ static struct dma_chan *sdma_xlate(struct of_phandle_args *dma_spec,
 	if (dma_spec->args_count != 3)
 		return NULL;
 
+	memset(&data, 0, sizeof(data));
+
 	data.dma_request = dma_spec->args[0];
 	data.peripheral_type = dma_spec->args[1];
-	data.priority = dma_spec->args[2];
+	data.priority = dma_spec->args[2] & 0xff;
 	/*
 	 * init dma_request2 to zero, which is not used by the dts.
 	 * For P2P, dma_request2 is init from dma_request_channel(),
@@ -2066,6 +2510,7 @@ static int sdma_probe(struct platform_device *pdev)
 
 		sdmac->channel = i;
 		sdmac->vc.desc_free = sdma_desc_free;
+		INIT_LIST_HEAD(&sdmac->terminated);
 		INIT_WORK(&sdmac->terminate_worker,
 				sdma_channel_terminate_work);
 		/*
@@ -2077,7 +2522,13 @@ static int sdma_probe(struct platform_device *pdev)
 			vchan_init(&sdmac->vc, &sdma->dma_device);
 	}
 
-	ret = sdma_init(sdma);
+	if (np) {
+		sdma->iram_pool = of_gen_pool_get(np, "iram", 0);
+		if (sdma->iram_pool)
+			dev_info(&pdev->dev, "alloc bd from iram.\n");
+	}
+
+	ret = sdma_init_sw(sdma);
 	if (ret)
 		goto err_init;
 
@@ -2133,16 +2584,8 @@ static int sdma_probe(struct platform_device *pdev)
 		of_node_put(spba_bus);
 	}
 
-	/*
-	 * Kick off firmware loading as the very last step:
-	 * attempt to load firmware only if we're not on the error path, because
-	 * the firmware callback requires a fully functional and allocated sdma
-	 * instance.
-	 */
 	if (pdata) {
-		ret = sdma_get_firmware(sdma, pdata->fw_name);
-		if (ret)
-			dev_warn(&pdev->dev, "failed to get firmware from platform data\n");
+		sdma->fw_name = pdata->fw_name;
 	} else {
 		/*
 		 * Because that device tree does not encode ROM script address,
@@ -2151,15 +2594,26 @@ static int sdma_probe(struct platform_device *pdev)
 		 */
 		ret = of_property_read_string(np, "fsl,sdma-ram-script-name",
 					      &fw_name);
-		if (ret) {
+		if (ret)
 			dev_warn(&pdev->dev, "failed to get firmware name\n");
-		} else {
-			ret = sdma_get_firmware(sdma, fw_name);
-			if (ret)
-				dev_warn(&pdev->dev, "failed to get firmware from device tree\n");
-		}
+		else
+			sdma->fw_name = fw_name;
+
+		ret = sdma_get_firmware(sdma, sdma->fw_name);
+		if (ret)
+			dev_warn(sdma->dev, "failed to get firmware.\n");
+	}
+
+	/* enable autosuspend for pm_runtime */
+	if (sdma->drvdata->pm_runtime) {
+		pm_runtime_set_autosuspend_delay(&pdev->dev, 8000);
+		pm_runtime_use_autosuspend(&pdev->dev);
+		pm_runtime_mark_last_busy(&pdev->dev);
+		pm_runtime_set_active(&pdev->dev);
 	}
 
+	pm_runtime_enable(&pdev->dev);
+
 	return 0;
 
 err_register:
@@ -2181,6 +2635,7 @@ static int sdma_remove(struct platform_device *pdev)
 	devm_free_irq(&pdev->dev, sdma->irq, sdma);
 	dma_async_device_unregister(&sdma->dma_device);
 	kfree(sdma->script_addrs);
+	kfree(sdma->fw_data);
 	clk_unprepare(sdma->clk_ahb);
 	clk_unprepare(sdma->clk_ipg);
 	/* Kill the tasklet */
@@ -2191,14 +2646,127 @@ static int sdma_remove(struct platform_device *pdev)
 		sdma_free_chan_resources(&sdmac->vc.chan);
 	}
 
+	pm_runtime_disable(&pdev->dev);
+	pm_runtime_set_suspended(&pdev->dev);
+	pm_runtime_dont_use_autosuspend(&pdev->dev);
+
 	platform_set_drvdata(pdev, NULL);
 	return 0;
 }
 
+#ifdef CONFIG_PM_SLEEP
+static int sdma_suspend(struct device *dev)
+{
+	struct platform_device *pdev = to_platform_device(dev);
+	struct sdma_engine *sdma = platform_get_drvdata(pdev);
+	int i, ret = 0;
+
+	/* Do nothing if not i.MX6SX or i.MX7D, i.MX8MP */
+	if (sdma->drvdata != &sdma_imx6sx && sdma->drvdata != &sdma_imx7d
+	   && sdma->drvdata != &sdma_imx6ul && sdma->drvdata != &sdma_imx8mp)
+		return 0;
+
+	if (!sdma->is_on)
+		return 0;
+
+	ret = sdma_save_restore_context(sdma, true);
+	if (ret) {
+		dev_err(sdma->dev, "save context error!\n");
+		return ret;
+	}
+
+	/* save regs */
+	for (i = 0; i < MXC_SDMA_SAVED_REG_NUM; i++) {
+		/*
+		 * 0x78(SDMA_XTRIG_CONF2+4)~0x100(SDMA_CHNPRI_O) registers are
+		 * reserved and can't be touched. Skip these regs.
+		 */
+		if (i > SDMA_XTRIG_CONF2 / 4)
+			sdma->save_regs[i] = readl_relaxed(sdma->regs +
+							   MXC_SDMA_RESERVED_REG
+							   + 4 * i);
+		else
+			sdma->save_regs[i] = readl_relaxed(sdma->regs + 4 * i);
+	}
+
+	if (sdma->drvdata->has_done0) {
+		for (i = 0; i < 2; i++)
+			sdma->save_done0_regs[i] =
+			readl_relaxed(sdma->regs + SDMA_DONE0_CONFIG + 4 * i);
+	}
+
+	return 0;
+}
+
+static int sdma_resume(struct device *dev)
+{
+	struct platform_device *pdev = to_platform_device(dev);
+	struct sdma_engine *sdma = platform_get_drvdata(pdev);
+	int i, ret;
+
+	/* Do nothing if not i.MX6SX or i.MX7D, i.MX8MP*/
+	if (sdma->drvdata != &sdma_imx6sx && sdma->drvdata != &sdma_imx7d
+	    && sdma->drvdata != &sdma_imx6ul && sdma->drvdata != &sdma_imx8mp)
+		return 0;
+
+	if (!sdma->is_on)
+		return 0;
+
+	/* Do nothing if mega/fast mix not turned off */
+	if (readl_relaxed(sdma->regs + SDMA_H_C0PTR))
+		return 0;
+
+	/* Firmware was lost, mark as "not ready" */
+	sdma->fw_loaded = false;
+
+	/* restore regs and load firmware */
+	for (i = 0; i < MXC_SDMA_SAVED_REG_NUM; i++) {
+		/*
+		 * 0x78(SDMA_XTRIG_CONF2+4)~0x100(SDMA_CHNPRI_O) registers are
+		 * reserved and can't be touched. Skip these regs.
+		 */
+		if (i > SDMA_XTRIG_CONF2 / 4)
+			writel_relaxed(sdma->save_regs[i], sdma->regs +
+				       MXC_SDMA_RESERVED_REG + 4 * i);
+		/* set static context switch  mode before channel0 running */
+		else if (i == SDMA_H_CONFIG / 4)
+			writel_relaxed(sdma->save_regs[i] & ~SDMA_H_CONFIG_CSM,
+					sdma->regs + SDMA_H_CONFIG);
+		else
+			writel_relaxed(sdma->save_regs[i], sdma->regs + 4 * i);
+	}
+
+	/* restore SDMA_DONEx_CONFIG */
+	if (sdma->drvdata->has_done0) {
+		for (i = 0; i < 2; i++)
+			writel_relaxed(sdma->save_done0_regs[i],
+				sdma->regs + SDMA_DONE0_CONFIG + 4 * i);
+	}
+
+	/* prepare priority for channel0 to start */
+	sdma_set_channel_priority(&sdma->channel[0], MXC_SDMA_MAX_PRIORITY);
+
+	if (sdma_load_script(sdma))
+		dev_warn(sdma->dev, "failed to load firmware.\n");
+
+	ret = sdma_save_restore_context(sdma, false);
+	if (ret)
+		dev_err(sdma->dev, "restore context error!\n");
+
+	return ret;
+}
+#endif
+
+static const struct dev_pm_ops sdma_pm_ops = {
+	SET_LATE_SYSTEM_SLEEP_PM_OPS(sdma_suspend, sdma_resume)
+	SET_RUNTIME_PM_OPS(sdma_runtime_suspend, sdma_runtime_resume, NULL)
+};
+
 static struct platform_driver sdma_driver = {
 	.driver		= {
 		.name	= "imx-sdma",
 		.of_match_table = sdma_dt_ids,
+		.pm = &sdma_pm_ops,
 	},
 	.id_table	= sdma_devtypes,
 	.remove		= sdma_remove,
diff --git a/drivers/dma/mxs-dma.c b/drivers/dma/mxs-dma.c
index 65f816b40..9067f543c 100644
--- a/drivers/dma/mxs-dma.c
+++ b/drivers/dma/mxs-dma.c
@@ -25,6 +25,8 @@
 #include <linux/of_dma.h>
 #include <linux/list.h>
 #include <linux/dma/mxs-dma.h>
+#include <linux/pm_runtime.h>
+#include <linux/dmapool.h>
 
 #include <asm/irq.h>
 
@@ -39,6 +41,8 @@
 #define dma_is_apbh(mxs_dma)	((mxs_dma)->type == MXS_DMA_APBH)
 #define apbh_is_old(mxs_dma)	((mxs_dma)->dev_id == IMX23_DMA)
 
+#define MXS_DMA_RPM_TIMEOUT 50 /* ms */
+
 #define HW_APBHX_CTRL0				0x000
 #define BM_APBH_CTRL0_APB_BURST8_EN		(1 << 29)
 #define BM_APBH_CTRL0_APB_BURST_EN		(1 << 28)
@@ -118,6 +122,7 @@ struct mxs_dma_chan {
 	enum dma_status			status;
 	unsigned int			flags;
 	bool				reset;
+	struct dma_pool			*ccw_pool;
 #define MXS_DMA_SG_LOOP			(1 << 0)
 #define MXS_DMA_USE_SEMAPHORE		(1 << 1)
 };
@@ -415,11 +420,13 @@ static int mxs_dma_alloc_chan_resources(struct dma_chan *chan)
 {
 	struct mxs_dma_chan *mxs_chan = to_mxs_dma_chan(chan);
 	struct mxs_dma_engine *mxs_dma = mxs_chan->mxs_dma;
+	struct device *dev = &mxs_dma->pdev->dev;
 	int ret;
 
-	mxs_chan->ccw = dma_alloc_coherent(mxs_dma->dma_device.dev,
-					   CCW_BLOCK_SIZE,
-					   &mxs_chan->ccw_phys, GFP_KERNEL);
+	mxs_chan->ccw = dma_pool_zalloc(mxs_chan->ccw_pool,
+				        GFP_ATOMIC,
+				        &mxs_chan->ccw_phys);
+
 	if (!mxs_chan->ccw) {
 		ret = -ENOMEM;
 		goto err_alloc;
@@ -430,9 +437,11 @@ static int mxs_dma_alloc_chan_resources(struct dma_chan *chan)
 	if (ret)
 		goto err_irq;
 
-	ret = clk_prepare_enable(mxs_dma->clk);
-	if (ret)
+	ret = pm_runtime_get_sync(dev);
+	if (ret < 0) {
+		dev_err(dev, "Failed to enable clock\n");
 		goto err_clk;
+	}
 
 	mxs_dma_reset_chan(chan);
 
@@ -447,8 +456,8 @@ static int mxs_dma_alloc_chan_resources(struct dma_chan *chan)
 err_clk:
 	free_irq(mxs_chan->chan_irq, mxs_dma);
 err_irq:
-	dma_free_coherent(mxs_dma->dma_device.dev, CCW_BLOCK_SIZE,
-			mxs_chan->ccw, mxs_chan->ccw_phys);
+	dma_pool_free(mxs_chan->ccw_pool, mxs_chan->ccw,
+		      mxs_chan->ccw_phys);
 err_alloc:
 	return ret;
 }
@@ -457,15 +466,18 @@ static void mxs_dma_free_chan_resources(struct dma_chan *chan)
 {
 	struct mxs_dma_chan *mxs_chan = to_mxs_dma_chan(chan);
 	struct mxs_dma_engine *mxs_dma = mxs_chan->mxs_dma;
+	struct device *dev = &mxs_dma->pdev->dev;
 
 	mxs_dma_disable_chan(chan);
 
 	free_irq(mxs_chan->chan_irq, mxs_dma);
 
-	dma_free_coherent(mxs_dma->dma_device.dev, CCW_BLOCK_SIZE,
-			mxs_chan->ccw, mxs_chan->ccw_phys);
+	dma_pool_free(mxs_chan->ccw_pool, mxs_chan->ccw,
+		      mxs_chan->ccw_phys);
+
+	pm_runtime_mark_last_busy(dev);
+	pm_runtime_put_autosuspend(dev);
 
-	clk_disable_unprepare(mxs_dma->clk);
 }
 
 /*
@@ -688,14 +700,32 @@ static enum dma_status mxs_dma_tx_status(struct dma_chan *chan,
 	return mxs_chan->status;
 }
 
-static int __init mxs_dma_init(struct mxs_dma_engine *mxs_dma)
+static int mxs_dma_init_rpm(struct mxs_dma_engine *mxs_dma)
+{
+	struct device *dev = &mxs_dma->pdev->dev;
+
+	pm_runtime_enable(dev);
+	pm_runtime_set_autosuspend_delay(dev, MXS_DMA_RPM_TIMEOUT);
+	pm_runtime_use_autosuspend(dev);
+
+	return 0;
+}
+
+static int mxs_dma_init(struct mxs_dma_engine *mxs_dma)
 {
+	struct device *dev = &mxs_dma->pdev->dev;
 	int ret;
 
-	ret = clk_prepare_enable(mxs_dma->clk);
+	ret = mxs_dma_init_rpm(mxs_dma);
 	if (ret)
 		return ret;
 
+	ret = pm_runtime_get_sync(dev);
+	if (ret < 0) {
+		dev_err(dev, "Failed to enable clock\n");
+		return ret;
+	}
+
 	ret = stmp_reset_block(mxs_dma->base);
 	if (ret)
 		goto err_out;
@@ -713,7 +743,8 @@ static int __init mxs_dma_init(struct mxs_dma_engine *mxs_dma)
 		mxs_dma->base + HW_APBHX_CTRL1 + STMP_OFFSET_REG_SET);
 
 err_out:
-	clk_disable_unprepare(mxs_dma->clk);
+	pm_runtime_mark_last_busy(dev);
+	pm_runtime_put_autosuspend(dev);
 	return ret;
 }
 
@@ -728,6 +759,12 @@ static bool mxs_dma_filter_fn(struct dma_chan *chan, void *fn_param)
 	struct mxs_dma_engine *mxs_dma = mxs_chan->mxs_dma;
 	int chan_irq;
 
+	if (strcmp(chan->device->dev->driver->name, "mxs-dma"))
+		return false;
+
+	if (!mxs_dma)
+		return false;
+
 	if (chan->chan_id != param->chan_id)
 		return false;
 
@@ -759,7 +796,7 @@ static struct dma_chan *mxs_dma_xlate(struct of_phandle_args *dma_spec,
 				     ofdma->of_node);
 }
 
-static int __init mxs_dma_probe(struct platform_device *pdev)
+static int mxs_dma_probe(struct platform_device *pdev)
 {
 	struct device_node *np = pdev->dev.of_node;
 	const struct platform_device_id *id_entry;
@@ -767,6 +804,7 @@ static int __init mxs_dma_probe(struct platform_device *pdev)
 	const struct mxs_dma_type *dma_type;
 	struct mxs_dma_engine *mxs_dma;
 	struct resource *iores;
+	struct dma_pool *ccw_pool;
 	int ret, i;
 
 	mxs_dma = devm_kzalloc(&pdev->dev, sizeof(*mxs_dma), GFP_KERNEL);
@@ -813,19 +851,31 @@ static int __init mxs_dma_probe(struct platform_device *pdev)
 
 		tasklet_setup(&mxs_chan->tasklet, mxs_dma_tasklet);
 
-
 		/* Add the channel to mxs_chan list */
 		list_add_tail(&mxs_chan->chan.device_node,
 			&mxs_dma->dma_device.channels);
 	}
 
+	platform_set_drvdata(pdev, mxs_dma);
+	mxs_dma->pdev = pdev;
+
 	ret = mxs_dma_init(mxs_dma);
 	if (ret)
 		return ret;
 
-	mxs_dma->pdev = pdev;
 	mxs_dma->dma_device.dev = &pdev->dev;
 
+	/* create the dma pool */
+	ccw_pool = dma_pool_create("ccw_pool",
+				   mxs_dma->dma_device.dev,
+				   CCW_BLOCK_SIZE, 32, 0);
+
+	for (i = 0; i < MXS_DMA_CHANNELS; i++) {
+		struct mxs_dma_chan *mxs_chan = &mxs_dma->mxs_chans[i];
+
+		mxs_chan->ccw_pool = ccw_pool;
+	}
+
 	/* mxs_dma gets 65535 bytes maximum sg size */
 	dma_set_max_seg_size(mxs_dma->dma_device.dev, MAX_XFER_BYTES);
 
@@ -860,16 +910,83 @@ static int __init mxs_dma_probe(struct platform_device *pdev)
 	return 0;
 }
 
+static int mxs_dma_remove(struct platform_device *pdev)
+{
+	struct mxs_dma_engine *mxs_dma = platform_get_drvdata(pdev);
+	int i;
+
+	dma_async_device_unregister(&mxs_dma->dma_device);
+	dma_pool_destroy(mxs_dma->mxs_chans[0].ccw_pool);
+
+	for (i = 0; i < MXS_DMA_CHANNELS; i++) {
+		struct mxs_dma_chan *mxs_chan = &mxs_dma->mxs_chans[i];
+
+		tasklet_kill(&mxs_chan->tasklet);
+		mxs_chan->ccw_pool = NULL;
+	}
+
+	return 0;
+}
+
+#ifdef CONFIG_PM_SLEEP
+static int mxs_dma_pm_suspend(struct device *dev)
+{
+	int ret;
+
+	ret = pm_runtime_force_suspend(dev);
+
+	return ret;
+}
+
+static int mxs_dma_pm_resume(struct device *dev)
+{
+	struct mxs_dma_engine *mxs_dma = dev_get_drvdata(dev);
+	int ret;
+
+	ret = mxs_dma_init(mxs_dma);
+	if (ret)
+		return ret;
+
+	return 0;
+}
+#endif
+
+int mxs_dma_runtime_suspend(struct device *dev)
+{
+	struct mxs_dma_engine *mxs_dma = dev_get_drvdata(dev);
+
+	clk_disable_unprepare(mxs_dma->clk);
+
+	return 0;
+}
+
+int mxs_dma_runtime_resume(struct device *dev)
+{
+	struct mxs_dma_engine *mxs_dma = dev_get_drvdata(dev);
+	int ret;
+
+	ret = clk_prepare_enable(mxs_dma->clk);
+	if (ret) {
+		dev_err(&mxs_dma->pdev->dev, "failed to enable the clock\n");
+		return ret;
+	}
+
+	return 0;
+}
+
+static const struct dev_pm_ops mxs_dma_pm_ops = {
+	SET_RUNTIME_PM_OPS(mxs_dma_runtime_suspend, mxs_dma_runtime_resume, NULL)
+	SET_SYSTEM_SLEEP_PM_OPS(mxs_dma_pm_suspend, mxs_dma_pm_resume)
+};
+
 static struct platform_driver mxs_dma_driver = {
 	.driver		= {
 		.name	= "mxs-dma",
+		.pm = &mxs_dma_pm_ops,
 		.of_match_table = mxs_dma_dt_ids,
 	},
 	.id_table	= mxs_dma_ids,
+	.remove		= mxs_dma_remove,
+	.probe = mxs_dma_probe,
 };
-
-static int __init mxs_dma_module_init(void)
-{
-	return platform_driver_probe(&mxs_dma_driver, mxs_dma_probe);
-}
-subsys_initcall(mxs_dma_module_init);
+module_platform_driver(mxs_dma_driver);
